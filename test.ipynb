{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data/data.csv\n",
    "有重複資料\n",
    "562 => 522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Features.dle import dle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "[[ 4.          6.          3.          0.93839836]\n",
      " [ 4.         10.          3.          0.92813142]\n",
      " [ 4.          4.          3.          0.92813142]\n",
      " [ 4.          4.          1.          0.92607803]\n",
      " [ 4.          8.          3.          0.92402464]\n",
      " [ 3.          4.          3.          0.90965092]\n",
      " [ 4.          8.          1.          0.89117043]\n",
      " [ 4.          6.          1.          0.88706366]\n",
      " [ 3.          6.          3.          0.88501027]\n",
      " [ 4.         10.          1.          0.88295688]\n",
      " [ 3.          8.          3.          0.88090349]\n",
      " [ 4.          6.          2.          0.87679671]\n",
      " [ 3.         10.          3.          0.86858316]\n",
      " [ 4.          4.          2.          0.85215606]\n",
      " [ 4.          8.          2.          0.85215606]\n",
      " [ 4.         10.          2.          0.85010267]\n",
      " [ 1.          8.          3.          0.81314168]\n",
      " [ 3.          4.          2.          0.79876797]\n",
      " [ 3.          6.          2.          0.79671458]\n",
      " [ 3.          6.          1.          0.77002053]]\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"data/linear_features/score_history/score_history.npy\")\n",
    "print(x.shape)\n",
    "print(x[x[:, 3].argsort()][::-1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Refseq_ID</th>\n",
       "      <th>Cytosolic</th>\n",
       "      <th>Nucleus</th>\n",
       "      <th>Gene_ID</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NR_002728.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NCBI:10984</td>\n",
       "      <td>AGGCAGAACGGTCGCCGCGTCGCCTCAGCACGGACCTCCAGGGAGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NR_003255.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NCBI:9383</td>\n",
       "      <td>TAAAAATGTAAAAGATCAGCTGGGTGTGGTGGCTCACACCTGTAAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NM_173600.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NCBI:283463</td>\n",
       "      <td>AGACTTGTCTCTCCAGAATGACTATCCTCCATTTCTAGGTCCCAAA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Refseq_ID  Cytosolic  Nucleus      Gene_ID  \\\n",
       "0           0  NR_002728.3       0.30     0.70   NCBI:10984   \n",
       "1           1  NR_003255.2       0.25     0.75    NCBI:9383   \n",
       "2           2  NM_173600.2       0.20     0.80  NCBI:283463   \n",
       "\n",
       "                                            Sequence  \n",
       "0  AGGCAGAACGGTCGCCGCGTCGCCTCAGCACGGACCTCCAGGGAGC...  \n",
       "1  TAAAAATGTAAAAGATCAGCTGGGTGTGGTGGCTCACACCTGTAAT...  \n",
       "2  AGACTTGTCTCTCCAGAATGACTATCCTCCATTTCTAGGTCCCAAA...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/society/cdhit80_data_seq.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lncATLAS/2021-11-25_lncATLAS_all_data_RCI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnrci = df[df[\"Data Type\"] == \"CNRCI\"]\n",
    "cnrci = cnrci[cnrci[\"Value\"].notna()]\n",
    "ensembl_id = cnrci[\"ENSEMBL ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSEMBL ID</th>\n",
       "      <th>Data Source</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Value</th>\n",
       "      <th>Gene Name</th>\n",
       "      <th>Coding Type</th>\n",
       "      <th>Biotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>A549</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>1.080680</td>\n",
       "      <td>TSPAN6</td>\n",
       "      <td>coding</td>\n",
       "      <td>coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>H1.hESC</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>1.857340</td>\n",
       "      <td>TSPAN6</td>\n",
       "      <td>coding</td>\n",
       "      <td>coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>HeLa.S3</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>1.868390</td>\n",
       "      <td>TSPAN6</td>\n",
       "      <td>coding</td>\n",
       "      <td>coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>HepG2</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>2.294360</td>\n",
       "      <td>TSPAN6</td>\n",
       "      <td>coding</td>\n",
       "      <td>coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>HT1080</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>0.866395</td>\n",
       "      <td>TSPAN6</td>\n",
       "      <td>coding</td>\n",
       "      <td>coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714484</th>\n",
       "      <td>ENSG00000283122</td>\n",
       "      <td>HepG2</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>-2.584960</td>\n",
       "      <td>HYMAI</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714485</th>\n",
       "      <td>ENSG00000283122</td>\n",
       "      <td>HT1080</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>-1.485430</td>\n",
       "      <td>HYMAI</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714487</th>\n",
       "      <td>ENSG00000283122</td>\n",
       "      <td>IMR.90</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>-3.305810</td>\n",
       "      <td>HYMAI</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714494</th>\n",
       "      <td>ENSG00000283122</td>\n",
       "      <td>MCF.7</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>-3.544320</td>\n",
       "      <td>HYMAI</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714499</th>\n",
       "      <td>ENSG00000283122</td>\n",
       "      <td>SK.N.SH</td>\n",
       "      <td>CNRCI</td>\n",
       "      <td>-2.611430</td>\n",
       "      <td>HYMAI</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198183 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ENSEMBL ID Data Source Data Type     Value Gene Name Coding Type  \\\n",
       "0       ENSG00000000003        A549     CNRCI  1.080680    TSPAN6      coding   \n",
       "2       ENSG00000000003     H1.hESC     CNRCI  1.857340    TSPAN6      coding   \n",
       "3       ENSG00000000003     HeLa.S3     CNRCI  1.868390    TSPAN6      coding   \n",
       "4       ENSG00000000003       HepG2     CNRCI  2.294360    TSPAN6      coding   \n",
       "5       ENSG00000000003      HT1080     CNRCI  0.866395    TSPAN6      coding   \n",
       "...                 ...         ...       ...       ...       ...         ...   \n",
       "714484  ENSG00000283122       HepG2     CNRCI -2.584960     HYMAI          nc   \n",
       "714485  ENSG00000283122      HT1080     CNRCI -1.485430     HYMAI          nc   \n",
       "714487  ENSG00000283122      IMR.90     CNRCI -3.305810     HYMAI          nc   \n",
       "714494  ENSG00000283122       MCF.7     CNRCI -3.544320     HYMAI          nc   \n",
       "714499  ENSG00000283122     SK.N.SH     CNRCI -2.611430     HYMAI          nc   \n",
       "\n",
       "       Biotype  \n",
       "0       coding  \n",
       "2       coding  \n",
       "3       coding  \n",
       "4       coding  \n",
       "5       coding  \n",
       "...        ...  \n",
       "714484      nc  \n",
       "714485      nc  \n",
       "714487      nc  \n",
       "714494      nc  \n",
       "714499      nc  \n",
       "\n",
       "[198183 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnrci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = []\n",
    "for eidx in ensembl_id:\n",
    "    if len(cnrci[cnrci[\"ENSEMBL ID\"] == eidx][\"Value\"]) > 1:\n",
    "        variance.append(statistics.variance(cnrci[cnrci[\"ENSEMBL ID\"] == eidx][\"Value\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "v_np = np.array(variance)\n",
    "plt.hist(v_np, bins='auto')\n",
    "plt.xlim(-0.5, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id   N   C\n",
      "0  ENSG00000000003  10   2\n",
      "0  ENSG00000000005   1   0\n",
      "0  ENSG00000000419  12   1\n",
      "0  ENSG00000000457   1  12\n",
      "0  ENSG00000000460   2  12\n",
      "0  ENSG00000000938   1   3\n",
      "0  ENSG00000000971   2   5\n",
      "0  ENSG00000001036  12   1\n",
      "0  ENSG00000001084   6   7\n",
      "0  ENSG00000001167   4  11\n"
     ]
    }
   ],
   "source": [
    "nc_count = pd.DataFrame([], columns=[\"id\", \"N\", \"C\"])\n",
    "for eidx in ensembl_id:\n",
    "    temp = pd.DataFrame([[eidx, len(cnrci[(cnrci[\"ENSEMBL ID\"] == eidx) & (cnrci[\"Value\"] > 0)]) , len(cnrci[(cnrci[\"ENSEMBL ID\"] == eidx) & (cnrci[\"Value\"] < 0)])]], columns=[\"id\", \"N\", \"C\"])\n",
    "    nc_count = nc_count.append(temp)\n",
    "print(nc_count.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_count = nc_count.groupby(by=[\"N\", \"C\"]).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_m = np.zeros((15,15))\n",
    "\n",
    "for n in range(15):\n",
    "    for c in range(15):\n",
    "        if len(nc_count[(nc_count[\"N\"] == n) & (nc_count[\"C\"] == c)]):\n",
    "            h_m[n, c] = int(list(nc_count[(nc_count[\"N\"] == n) & (nc_count[\"C\"] == c)][\"id\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bottom = plt.cm.get_cmap('Blues', 128)\n",
    "newcolors = np.vstack((bottom(0),\n",
    "                       bottom(np.linspace(0.3, 1, 128))))\n",
    "newcmp = matplotlib.colors.ListedColormap(newcolors, name='OrangeBlue')\n",
    "im = ax.imshow(h_m, cmap=newcmp)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel(\"cbarlabel\", rotation=-90, va=\"bottom\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1    2         3\n",
      "0   1.0   4.0  0.0  0.552361\n",
      "1   1.0   4.0  1.0  0.577002\n",
      "2   1.0   4.0  2.0  0.535934\n",
      "3   1.0   4.0  3.0  0.542094\n",
      "4   1.0   6.0  0.0  0.546201\n",
      "..  ...   ...  ...       ...\n",
      "59  4.0   8.0  3.0  0.924025\n",
      "60  4.0  10.0  0.0  0.683778\n",
      "61  4.0  10.0  1.0  0.882957\n",
      "62  4.0  10.0  2.0  0.850103\n",
      "63  4.0  10.0  3.0  0.928131\n",
      "\n",
      "[64 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "score_history = np.load(\"data/linear_features/score_history/score_history.npy\")\n",
    "# score_history = score_history[score_history[:, 3].argsort()]\n",
    "score_history = pd.DataFrame(score_history, columns=[\"k\", \"power\", \"normalization\", \"acc\", \"f1_score_macro\"])\n",
    "print(score_history)\n",
    "# score_history.to_csv(\"data/linear_features/score_history/dle_point_score_history_1123.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/linear_features/cdhit80_data_seq_loc75_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = df[\"Sequence\"]\n",
    "data_y = np.where(df[\"loc\"].to_numpy() == \"Cytosolic\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2432, 37)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"data/linear_features/cdhit80_loc75_k1_power8_nor3.npy\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "for i in range(64):\n",
    "    if x is None:\n",
    "        x = np.array([data_x[:, i*10 + 4]]).T\n",
    "        # x = np.array([data_x[:, i*10], data_x[:, i*10 + 9]]).T\n",
    "        # x = np.array([np.average(data_x[:, i*10:(i+1)*10], axis=1)]).T\n",
    "    else:\n",
    "        x = np.append(x, np.array([data_x[:, i*10 + 4]]).T, axis=1)\n",
    "        # x = np.append(x, np.array([data_x[:, i*10], data_x[:, i*10 + 9]]).T, axis=1)\n",
    "        # x = np.append(x, np.array([np.average(data_x[:, i*10:(i+1)*10], axis=1)]).T, axis=1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, data_y, test_size=0.20, shuffle=True, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([ 187, 2245]))\n",
      "(array([0, 1]), array([ 147, 1798]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(data_y, return_counts=True))\n",
    "print(np.unique(y_train, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', class_weight='balanced')\n",
    "scores = cross_val_score(clf, x, data_y, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: [0.85343261 0.80478554 0.85592885 0.83167797 0.72709271]\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This SVC instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dde7542de34b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"macro:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jand/.conda/envs/jand_venv/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \"\"\"\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_ties\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function_shape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ovo'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             raise ValueError(\"break_ties must be False when \"\n",
      "\u001b[0;32m/home/jand/.conda/envs/jand_venv/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jand/.conda/envs/jand_venv/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This SVC instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "print(\"score:\", scores)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"macro:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.971252566735113\n",
      "macro: 0.9175773694390716\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1, class_weight='balanced').fit(x_train, y_train)\n",
    "            \n",
    "score = clf.score(x_test, y_test)\n",
    "print(\"score:\", score)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"macro:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([ 40, 447]))\n",
      "(447, 37)\n",
      "(40, 37)\n",
      "1.0\n",
      "0.9686800894854586\n",
      "macro: 0.9175773694390716\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_test, return_counts=True))\n",
    "print(x_test[y_test == 1].shape)\n",
    "print(x_test[y_test == 0].shape)\n",
    "print(clf.score(x_test[y_test == 0], y_test[y_test == 0]))\n",
    "print(clf.score(x_test[y_test == 1], y_test[y_test == 1]))\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"macro:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/k_mers/model/k1p8n3_svm.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.9175773694390716\n"
     ]
    }
   ],
   "source": [
    "with open('data/k_mers/model/k1p8n3_svm.pickle', 'rb') as f:\n",
    "    clf2 = pickle.load(f)\n",
    "y_pred = clf2.predict(x_test)\n",
    "print(\"macro:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, nor=2, num=1\n",
      "(2432, 4)\n",
      "Accuracy = 41.0678% (200/487) (classification)\n",
      "score: 41.067761806981515\n",
      "macro: 0.3470721509821783\n",
      "[[0.04928131 0.03285421]\n",
      " [0.55646817 0.3613963 ]]\n",
      "k=1, nor=2, num=2\n",
      "(2432, 8)\n",
      "Accuracy = 66.7351% (325/487) (classification)\n",
      "score: 66.73511293634496\n",
      "macro: 0.44798488664987407\n",
      "[[0.01848049 0.06365503]\n",
      " [0.26899384 0.64887064]]\n",
      "k=1, nor=2, num=3\n",
      "(2432, 12)\n",
      "Accuracy = 29.7741% (145/487) (classification)\n",
      "score: 29.774127310061605\n",
      "macro: 0.2699610772143909\n",
      "[[0.0513347  0.03080082]\n",
      " [0.67145791 0.24640657]]\n",
      "k=1, nor=2, num=5\n",
      "(2432, 20)\n",
      "Accuracy = 19.3018% (94/487) (classification)\n",
      "score: 19.301848049281315\n",
      "macro: 0.1907117758242316\n",
      "[[0.0698152  0.01232033]\n",
      " [0.79466119 0.12320329]]\n",
      "k=1, nor=2, num=8\n",
      "(2432, 32)\n",
      "Accuracy = 13.1417% (64/487) (classification)\n",
      "score: 13.141683778234087\n",
      "macro: 0.13105045366538856\n",
      "[[0.07597536 0.00616016]\n",
      " [0.862423   0.05544148]]\n",
      "k=1, nor=2, num=10\n",
      "(2432, 40)\n",
      "Accuracy = 9.65092% (47/487) (classification)\n",
      "score: 9.650924024640657\n",
      "macro: 0.09283343493869811\n",
      "[[0.08008214 0.00205339]\n",
      " [0.90143737 0.0164271 ]]\n",
      "k=1, nor=3, num=1\n",
      "(2432, 4)\n",
      "Accuracy = 8.21355% (40/487) (classification)\n",
      "score: 8.213552361396303\n",
      "macro: 0.07590132827324479\n",
      "[[0.08213552 0.        ]\n",
      " [0.91786448 0.        ]]\n",
      "k=1, nor=3, num=2\n",
      "(2432, 8)\n",
      "Accuracy = 20.1232% (98/487) (classification)\n",
      "score: 20.123203285420946\n",
      "macro: 0.1977683203116729\n",
      "[[0.06776181 0.01437372]\n",
      " [0.78439425 0.13347023]]\n",
      "k=1, nor=3, num=3\n",
      "(2432, 12)\n",
      "Accuracy = 24.6407% (120/487) (classification)\n",
      "score: 24.64065708418891\n",
      "macro: 0.23399121397192757\n",
      "[[0.05954825 0.02258727]\n",
      " [0.73100616 0.18685832]]\n",
      "k=1, nor=3, num=5\n",
      "(2432, 20)\n",
      "Accuracy = 43.5318% (212/487) (classification)\n",
      "score: 43.53182751540041\n",
      "macro: 0.36308988790762486\n",
      "[[0.04928131 0.03285421]\n",
      " [0.53182752 0.38603696]]\n",
      "k=1, nor=3, num=8\n",
      "(2432, 32)\n",
      "Accuracy = 49.4867% (241/487) (classification)\n",
      "score: 49.48665297741273\n",
      "macro: 0.39603750756200845\n",
      "[[0.04517454 0.03696099]\n",
      " [0.46817248 0.44969199]]\n",
      "k=1, nor=3, num=10\n",
      "(2432, 40)\n",
      "Accuracy = 18.2752% (89/487) (classification)\n",
      "score: 18.275154004106774\n",
      "macro: 0.18175447483958124\n",
      "[[0.07392197 0.00821355]\n",
      " [0.80903491 0.10882957]]\n",
      "k=2, nor=2, num=1\n",
      "(2432, 16)\n",
      "Accuracy = 75.5647% (368/487) (classification)\n",
      "score: 75.564681724846\n",
      "macro: 0.4612481291078285\n",
      "[[0.00821355 0.07392197]\n",
      " [0.17043121 0.74743326]]\n",
      "k=2, nor=2, num=2\n",
      "(2432, 32)\n",
      "Accuracy = 89.117% (434/487) (classification)\n",
      "score: 89.11704312114989\n",
      "macro: 0.4893461272133742\n",
      "[[0.00205339 0.08008214]\n",
      " [0.02874743 0.88911704]]\n",
      "k=2, nor=2, num=3\n",
      "(2432, 48)\n",
      "Accuracy = 69.1992% (337/487) (classification)\n",
      "score: 69.19917864476386\n",
      "macro: 0.4605193193902871\n",
      "[[0.01848049 0.06365503]\n",
      " [0.24435318 0.67351129]]\n",
      "k=2, nor=2, num=5\n",
      "(2432, 80)\n",
      "Accuracy = 31.4168% (153/487) (classification)\n",
      "score: 31.416837782340863\n",
      "macro: 0.2807326305362955\n",
      "[[0.04928131 0.03285421]\n",
      " [0.65297741 0.26488706]]\n",
      "k=2, nor=2, num=8\n",
      "(2432, 128)\n",
      "Accuracy = 25.2567% (123/487) (classification)\n",
      "score: 25.256673511293638\n",
      "macro: 0.24065316473047532\n",
      "[[0.06365503 0.01848049]\n",
      " [0.72895277 0.1889117 ]]\n",
      "k=2, nor=2, num=10\n",
      "(2432, 160)\n",
      "Accuracy = 23.2033% (113/487) (classification)\n",
      "score: 23.203285420944557\n",
      "macro: 0.224812734082397\n",
      "[[0.06776181 0.01437372]\n",
      " [0.75359343 0.16427105]]\n",
      "k=2, nor=3, num=1\n",
      "(2432, 16)\n",
      "Accuracy = 42.7105% (208/487) (classification)\n",
      "score: 42.71047227926078\n",
      "macro: 0.35970877217784686\n",
      "[[0.0513347  0.03080082]\n",
      " [0.54209446 0.37577002]]\n",
      "k=2, nor=3, num=2\n",
      "(2432, 32)\n",
      "Accuracy = 34.7023% (169/487) (classification)\n",
      "score: 34.70225872689938\n",
      "macro: 0.30096955909435597\n",
      "[[0.04517454 0.03696099]\n",
      " [0.61601643 0.30184805]]\n",
      "k=2, nor=3, num=3\n",
      "(2432, 48)\n",
      "Accuracy = 32.6489% (159/487) (classification)\n",
      "score: 32.648870636550306\n",
      "macro: 0.2923267765373028\n",
      "[[0.05338809 0.02874743]\n",
      " [0.64476386 0.27310062]]\n",
      "k=2, nor=3, num=5\n",
      "(2432, 80)\n",
      "Accuracy = 31.4168% (153/487) (classification)\n",
      "score: 31.416837782340863\n",
      "macro: 0.2904714544946957\n",
      "[[0.06570842 0.0164271 ]\n",
      " [0.66940452 0.24845996]]\n",
      "k=2, nor=3, num=8\n",
      "(2432, 128)\n",
      "Accuracy = 33.0595% (161/487) (classification)\n",
      "score: 33.059548254620125\n",
      "macro: 0.3029416930101862\n",
      "[[0.06570842 0.0164271 ]\n",
      " [0.65297741 0.26488706]]\n",
      "k=2, nor=3, num=10\n",
      "(2432, 160)\n",
      "Accuracy = 36.961% (180/487) (classification)\n",
      "score: 36.96098562628337\n",
      "macro: 0.3288608379083266\n",
      "[[0.06160164 0.02053388]\n",
      " [0.60985626 0.30800821]]\n",
      "k=3, nor=2, num=1\n",
      "(2432, 64)\n",
      "Accuracy = 76.5914% (373/487) (classification)\n",
      "score: 76.59137577002053\n",
      "macro: 0.506769722814499\n",
      "[[0.02053388 0.06160164]\n",
      " [0.1724846  0.74537988]]\n",
      "k=3, nor=2, num=2\n",
      "(2432, 128)\n",
      "Accuracy = 68.9938% (336/487) (classification)\n",
      "score: 68.99383983572895\n",
      "macro: 0.46445711622351854\n",
      "[[0.02053388 0.06160164]\n",
      " [0.24845996 0.66940452]]\n",
      "k=3, nor=2, num=3\n",
      "(2432, 192)\n",
      "Accuracy = 49.2813% (240/487) (classification)\n",
      "score: 49.28131416837782\n",
      "macro: 0.38742138364779877\n",
      "[[0.03901437 0.04312115]\n",
      " [0.46406571 0.45379877]]\n",
      "k=3, nor=2, num=5\n",
      "(2432, 320)\n",
      "Accuracy = 27.1047% (132/487) (classification)\n",
      "score: 27.104722792607806\n",
      "macro: 0.2518553778912521\n",
      "[[0.05544148 0.02669405]\n",
      " [0.70225873 0.21560575]]\n",
      "k=3, nor=2, num=8\n",
      "(2432, 512)\n",
      "Accuracy = 26.694% (130/487) (classification)\n",
      "score: 26.69404517453799\n",
      "macro: 0.2466102466102466\n",
      "[[0.0513347  0.03080082]\n",
      " [0.70225873 0.21560575]]\n",
      "k=3, nor=2, num=10\n",
      "(2432, 640)\n",
      "Accuracy = 29.9795% (146/487) (classification)\n",
      "score: 29.97946611909651\n",
      "macro: 0.2714857887371519\n",
      "[[0.0513347  0.03080082]\n",
      " [0.66940452 0.24845996]]\n",
      "k=3, nor=3, num=1\n",
      "(2432, 64)\n",
      "Accuracy = 67.3511% (328/487) (classification)\n",
      "score: 67.35112936344969\n",
      "macro: 0.4605212738533996\n",
      "[[0.02258727 0.05954825]\n",
      " [0.26694045 0.65092402]]\n",
      "k=3, nor=3, num=2\n",
      "(2432, 128)\n",
      "Accuracy = 44.9692% (219/487) (classification)\n",
      "score: 44.969199178644764\n",
      "macro: 0.374348059518331\n",
      "[[0.0513347  0.03080082]\n",
      " [0.51950719 0.39835729]]\n",
      "k=3, nor=3, num=3\n",
      "(2432, 192)\n",
      "Accuracy = 59.1376% (288/487) (classification)\n",
      "score: 59.13757700205339\n",
      "macro: 0.4386833706915026\n",
      "[[0.0349076  0.04722793]\n",
      " [0.3613963  0.55646817]]\n",
      "k=3, nor=3, num=5\n",
      "(2432, 320)\n",
      "Accuracy = 50.7187% (247/487) (classification)\n",
      "score: 50.71868583162218\n",
      "macro: 0.3703943115707822\n",
      "[[0.02053388 0.06160164]\n",
      " [0.4312115  0.48665298]]\n",
      "k=3, nor=3, num=8\n",
      "(2432, 512)\n",
      "Accuracy = 79.8768% (389/487) (classification)\n",
      "score: 79.87679671457906\n",
      "macro: 0.5131986944104447\n",
      "[[0.0164271  0.06570842]\n",
      " [0.13552361 0.78234086]]\n",
      "k=3, nor=3, num=10\n",
      "(2432, 640)\n",
      "Accuracy = 70.2259% (342/487) (classification)\n",
      "score: 70.2258726899384\n",
      "macro: 0.4809438053585211\n",
      "[[0.02464066 0.05749487]\n",
      " [0.24024641 0.67761807]]\n",
      "k=4, nor=2, num=1\n",
      "(2432, 256)\n",
      "Accuracy = 52.9774% (258/487) (classification)\n",
      "score: 52.977412731006154\n",
      "macro: 0.3977751977751978\n",
      "[[0.03080082 0.0513347 ]\n",
      " [0.41889117 0.49897331]]\n",
      "k=4, nor=2, num=2\n",
      "(2432, 512)\n",
      "Accuracy = 47.0226% (229/487) (classification)\n",
      "score: 47.02258726899384\n",
      "macro: 0.391674413200093\n",
      "[[0.05544148 0.02669405]\n",
      " [0.50308008 0.41478439]]\n",
      "k=4, nor=2, num=3\n",
      "(2432, 768)\n",
      "Accuracy = 38.6037% (188/487) (classification)\n",
      "score: 38.60369609856263\n",
      "macro: 0.3373546610844388\n",
      "[[0.05749487 0.02464066]\n",
      " [0.58932238 0.32854209]]\n",
      "k=4, nor=2, num=5\n",
      "(2432, 1280)\n",
      "Accuracy = 36.1396% (176/487) (classification)\n",
      "score: 36.139630390143736\n",
      "macro: 0.3201163537116924\n",
      "[[0.05749487 0.02464066]\n",
      " [0.61396304 0.30390144]]\n",
      "k=4, nor=2, num=8\n",
      "(2432, 2048)\n",
      "Accuracy = 40.6571% (198/487) (classification)\n",
      "score: 40.6570841889117\n",
      "macro: 0.35311096709549616\n",
      "[[0.05954825 0.02258727]\n",
      " [0.57084189 0.34702259]]\n",
      "k=4, nor=2, num=10\n",
      "(2432, 2560)\n",
      "Accuracy = 42.7105% (208/487) (classification)\n",
      "score: 42.71047227926078\n",
      "macro: 0.3670581220390462\n",
      "[[0.05954825 0.02258727]\n",
      " [0.55030801 0.36755647]]\n",
      "k=4, nor=3, num=1\n",
      "(2432, 256)\n",
      "Accuracy = 73.7166% (359/487) (classification)\n",
      "score: 73.71663244353182\n",
      "macro: 0.49566343042071204\n",
      "[[0.02258727 0.05954825]\n",
      " [0.20328542 0.71457906]]\n",
      "k=4, nor=3, num=2\n",
      "(2432, 512)\n",
      "Accuracy = 73.306% (357/487) (classification)\n",
      "score: 73.30595482546201\n",
      "macro: 0.4512343110741281\n",
      "[[0.00821355 0.07392197]\n",
      " [0.19301848 0.724846  ]]\n",
      "k=4, nor=3, num=3\n",
      "(2432, 768)\n",
      "Accuracy = 71.0472% (346/487) (classification)\n",
      "score: 71.04722792607802\n",
      "macro: 0.45334479711494835\n",
      "[[0.01232033 0.0698152 ]\n",
      " [0.21971253 0.69815195]]\n",
      "k=4, nor=3, num=5\n",
      "(2432, 1280)\n",
      "Accuracy = 71.0472% (346/487) (classification)\n",
      "score: 71.04722792607802\n",
      "macro: 0.4753959340835645\n",
      "[[0.02053388 0.06160164]\n",
      " [0.22792608 0.6899384 ]]\n",
      "k=4, nor=3, num=8\n",
      "(2432, 2048)\n",
      "Accuracy = 77.0021% (375/487) (classification)\n",
      "score: 77.00205338809035\n",
      "macro: 0.49630614657210403\n",
      "[[0.0164271  0.06570842]\n",
      " [0.16427105 0.75359343]]\n",
      "k=4, nor=3, num=10\n",
      "(2432, 2560)\n",
      "Accuracy = 79.0554% (385/487) (classification)\n",
      "score: 79.05544147843942\n",
      "macro: 0.5009042681456475\n",
      "[[0.01437372 0.06776181]\n",
      " [0.14168378 0.7761807 ]]\n",
      "[[1.00000000e+00 2.00000000e+00 1.00000000e+00 4.00000000e+00\n",
      "  4.10677618e+01 3.47072151e-01]\n",
      " [1.00000000e+00 2.00000000e+00 2.00000000e+00 8.00000000e+00\n",
      "  6.67351129e+01 4.47984887e-01]\n",
      " [1.00000000e+00 2.00000000e+00 3.00000000e+00 1.20000000e+01\n",
      "  2.97741273e+01 2.69961077e-01]\n",
      " [1.00000000e+00 2.00000000e+00 5.00000000e+00 2.00000000e+01\n",
      "  1.93018480e+01 1.90711776e-01]\n",
      " [1.00000000e+00 2.00000000e+00 8.00000000e+00 3.20000000e+01\n",
      "  1.31416838e+01 1.31050454e-01]\n",
      " [1.00000000e+00 2.00000000e+00 1.00000000e+01 4.00000000e+01\n",
      "  9.65092402e+00 9.28334349e-02]\n",
      " [1.00000000e+00 3.00000000e+00 1.00000000e+00 4.00000000e+00\n",
      "  8.21355236e+00 7.59013283e-02]\n",
      " [1.00000000e+00 3.00000000e+00 2.00000000e+00 8.00000000e+00\n",
      "  2.01232033e+01 1.97768320e-01]\n",
      " [1.00000000e+00 3.00000000e+00 3.00000000e+00 1.20000000e+01\n",
      "  2.46406571e+01 2.33991214e-01]\n",
      " [1.00000000e+00 3.00000000e+00 5.00000000e+00 2.00000000e+01\n",
      "  4.35318275e+01 3.63089888e-01]\n",
      " [1.00000000e+00 3.00000000e+00 8.00000000e+00 3.20000000e+01\n",
      "  4.94866530e+01 3.96037508e-01]\n",
      " [1.00000000e+00 3.00000000e+00 1.00000000e+01 4.00000000e+01\n",
      "  1.82751540e+01 1.81754475e-01]\n",
      " [2.00000000e+00 2.00000000e+00 1.00000000e+00 1.60000000e+01\n",
      "  7.55646817e+01 4.61248129e-01]\n",
      " [2.00000000e+00 2.00000000e+00 2.00000000e+00 3.20000000e+01\n",
      "  8.91170431e+01 4.89346127e-01]\n",
      " [2.00000000e+00 2.00000000e+00 3.00000000e+00 4.80000000e+01\n",
      "  6.91991786e+01 4.60519319e-01]\n",
      " [2.00000000e+00 2.00000000e+00 5.00000000e+00 8.00000000e+01\n",
      "  3.14168378e+01 2.80732631e-01]\n",
      " [2.00000000e+00 2.00000000e+00 8.00000000e+00 1.28000000e+02\n",
      "  2.52566735e+01 2.40653165e-01]\n",
      " [2.00000000e+00 2.00000000e+00 1.00000000e+01 1.60000000e+02\n",
      "  2.32032854e+01 2.24812734e-01]\n",
      " [2.00000000e+00 3.00000000e+00 1.00000000e+00 1.60000000e+01\n",
      "  4.27104723e+01 3.59708772e-01]\n",
      " [2.00000000e+00 3.00000000e+00 2.00000000e+00 3.20000000e+01\n",
      "  3.47022587e+01 3.00969559e-01]\n",
      " [2.00000000e+00 3.00000000e+00 3.00000000e+00 4.80000000e+01\n",
      "  3.26488706e+01 2.92326777e-01]\n",
      " [2.00000000e+00 3.00000000e+00 5.00000000e+00 8.00000000e+01\n",
      "  3.14168378e+01 2.90471454e-01]\n",
      " [2.00000000e+00 3.00000000e+00 8.00000000e+00 1.28000000e+02\n",
      "  3.30595483e+01 3.02941693e-01]\n",
      " [2.00000000e+00 3.00000000e+00 1.00000000e+01 1.60000000e+02\n",
      "  3.69609856e+01 3.28860838e-01]\n",
      " [3.00000000e+00 2.00000000e+00 1.00000000e+00 6.40000000e+01\n",
      "  7.65913758e+01 5.06769723e-01]\n",
      " [3.00000000e+00 2.00000000e+00 2.00000000e+00 1.28000000e+02\n",
      "  6.89938398e+01 4.64457116e-01]\n",
      " [3.00000000e+00 2.00000000e+00 3.00000000e+00 1.92000000e+02\n",
      "  4.92813142e+01 3.87421384e-01]\n",
      " [3.00000000e+00 2.00000000e+00 5.00000000e+00 3.20000000e+02\n",
      "  2.71047228e+01 2.51855378e-01]\n",
      " [3.00000000e+00 2.00000000e+00 8.00000000e+00 5.12000000e+02\n",
      "  2.66940452e+01 2.46610247e-01]\n",
      " [3.00000000e+00 2.00000000e+00 1.00000000e+01 6.40000000e+02\n",
      "  2.99794661e+01 2.71485789e-01]\n",
      " [3.00000000e+00 3.00000000e+00 1.00000000e+00 6.40000000e+01\n",
      "  6.73511294e+01 4.60521274e-01]\n",
      " [3.00000000e+00 3.00000000e+00 2.00000000e+00 1.28000000e+02\n",
      "  4.49691992e+01 3.74348060e-01]\n",
      " [3.00000000e+00 3.00000000e+00 3.00000000e+00 1.92000000e+02\n",
      "  5.91375770e+01 4.38683371e-01]\n",
      " [3.00000000e+00 3.00000000e+00 5.00000000e+00 3.20000000e+02\n",
      "  5.07186858e+01 3.70394312e-01]\n",
      " [3.00000000e+00 3.00000000e+00 8.00000000e+00 5.12000000e+02\n",
      "  7.98767967e+01 5.13198694e-01]\n",
      " [3.00000000e+00 3.00000000e+00 1.00000000e+01 6.40000000e+02\n",
      "  7.02258727e+01 4.80943805e-01]\n",
      " [4.00000000e+00 2.00000000e+00 1.00000000e+00 2.56000000e+02\n",
      "  5.29774127e+01 3.97775198e-01]\n",
      " [4.00000000e+00 2.00000000e+00 2.00000000e+00 5.12000000e+02\n",
      "  4.70225873e+01 3.91674413e-01]\n",
      " [4.00000000e+00 2.00000000e+00 3.00000000e+00 7.68000000e+02\n",
      "  3.86036961e+01 3.37354661e-01]\n",
      " [4.00000000e+00 2.00000000e+00 5.00000000e+00 1.28000000e+03\n",
      "  3.61396304e+01 3.20116354e-01]\n",
      " [4.00000000e+00 2.00000000e+00 8.00000000e+00 2.04800000e+03\n",
      "  4.06570842e+01 3.53110967e-01]\n",
      " [4.00000000e+00 2.00000000e+00 1.00000000e+01 2.56000000e+03\n",
      "  4.27104723e+01 3.67058122e-01]\n",
      " [4.00000000e+00 3.00000000e+00 1.00000000e+00 2.56000000e+02\n",
      "  7.37166324e+01 4.95663430e-01]\n",
      " [4.00000000e+00 3.00000000e+00 2.00000000e+00 5.12000000e+02\n",
      "  7.33059548e+01 4.51234311e-01]\n",
      " [4.00000000e+00 3.00000000e+00 3.00000000e+00 7.68000000e+02\n",
      "  7.10472279e+01 4.53344797e-01]\n",
      " [4.00000000e+00 3.00000000e+00 5.00000000e+00 1.28000000e+03\n",
      "  7.10472279e+01 4.75395934e-01]\n",
      " [4.00000000e+00 3.00000000e+00 8.00000000e+00 2.04800000e+03\n",
      "  7.70020534e+01 4.96306147e-01]\n",
      " [4.00000000e+00 3.00000000e+00 1.00000000e+01 2.56000000e+03\n",
      "  7.90554415e+01 5.00904268e-01]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from libsvm.svmutil import *\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "k_array = [1]\n",
    "nor_array = [2]\n",
    "num_array = [1]\n",
    "k_array = [1, 2, 3, 4]\n",
    "nor_array = [2, 3]\n",
    "num_array = [1, 2, 3, 5, 8, 10]\n",
    "\n",
    "data_y = np.load(\"data/linear_features/data_y.npy\")\n",
    "\n",
    "score_history = None\n",
    "start_time = time.time()\n",
    "for k in k_array:\n",
    "    for nor in nor_array:\n",
    "        for num in num_array:\n",
    "            print(\"k=%s, nor=%s, num=%s\" % (k, nor, num))\n",
    "            \n",
    "            data_x = np.load(\"data/linear_features/point/k\" + str(k) + \"nor\" + str(nor) + \"n\" + str(num) + \".npy\")\n",
    "            data_x = data_x.reshape(data_x.shape[0],-1)\n",
    "            print(data_x.shape)\n",
    "            \n",
    "            x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.20, shuffle=True, random_state=12)\n",
    "            \n",
    "            prob = svm_problem(y_train, x_train, isKernel=True)\n",
    "            param = svm_parameter('-t 0 -w0 1798 -w1 147')\n",
    "            m = svm_train(prob, param)\n",
    "            p_label, p_acc, p_val = svm_predict(y_test, x_test, m)\n",
    "            (score, MSE, SCC) = evaluations(y_test, p_label)\n",
    "            macro = f1_score(y_test, p_label, average='macro')\n",
    "            print(\"score:\", score)\n",
    "            print(\"macro:\", macro)\n",
    "            print(metrics.confusion_matrix(y_test, p_label) / len(y_test))\n",
    "            # if macro >= 0.6:\n",
    "            #     with open(\"data/linear_features/model/k\" + str(k) + \"nor\" + str(nor) + \"n\" + str(num) + \"point_1122_\" + str(start_time) + \".pickle\", 'wb') as f:\n",
    "            #         pickle.dump(clf, f)\n",
    "            \n",
    "            if score_history is None:\n",
    "                score_history = np.array([[k, nor, num, data_x.shape[1], score, macro]])\n",
    "            else:\n",
    "                score_history = np.append(score_history, [[k, nor, num, data_x.shape[1], score, macro]], axis=0)\n",
    "print(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([ 147, 1798]))\n",
      "Accuracy = 41.0678% (200/487) (classification)\n",
      "0.3470721509821783\n"
     ]
    }
   ],
   "source": [
    "from libsvm.svmutil import *\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "# x_res, y_res = sm.fit_resample(x_train, y_train)\n",
    "# prob = svm_problem(y_res, x_res, isKernel=True)\n",
    "prob = svm_problem(y_train, x_train, isKernel=True)\n",
    "param = svm_parameter('-t 0 -w0 1798 -w1 147')\n",
    "m = svm_train(prob, param)\n",
    "p_label, p_acc, p_val = svm_predict(y_test, x_test, m)\n",
    "print(f1_score(y_test, p_label, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy = 92.3109%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.3108552631579"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libsvm.svmutil import *\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "prob = svm_problem(data_y, x, isKernel=True)\n",
    "param = svm_parameter('-v 10')\n",
    "svm_train(prob, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2432, 16, 11)\n"
     ]
    }
   ],
   "source": [
    "f_array = np.load(\"data/linear_features/k2_power10.npy\")\n",
    "vocab = np.load(\"data/linear_features/k2_vocab.npy\")\n",
    "print(f_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2432, 160)\n"
     ]
    }
   ],
   "source": [
    "token_out = 10\n",
    "data = []\n",
    "for f in f_array:\n",
    "    f_p = np.array([])\n",
    "    for z in f:\n",
    "        p = np.poly1d(z)\n",
    "        f_p = np.append(f_p, p(np.linspace(0, 1.0, num=token_out)))\n",
    "    data.append(f_p)\n",
    "x = np.array(data)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "nor_str = \"_norL\"\n",
    "data_df = pd.read_csv(\"data/linear_features/old/cdhit80_k\" + str(k) + \"_linear10\" + nor_str + \"_loc75.csv\")\n",
    "vocab = np.load(\"data/linear_features/old/cdhit80_k\" + str(k) + \"_vocab.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1945, 176)\n",
      "[ 142 1803]\n",
      "(487, 176)\n",
      "(array([0, 1]), array([ 38, 449]))\n"
     ]
    }
   ],
   "source": [
    "col = data_df.columns\n",
    "data_x = data_df[col[:-2]].to_numpy()\n",
    "data_y = data_df[col[-1]].to_numpy()\n",
    "data_y = np.where(data_y == \"Cytosolic\", 1, 0)\n",
    "\n",
    "idx = np.arange(len(data_y))\n",
    "np.random.shuffle(idx)\n",
    "x_train = data_x[idx[:int(len(idx) * 0.8)]]\n",
    "y_train = data_y[idx[:int(len(idx) * 0.8)]]\n",
    "x_test = data_x[idx[int(len(idx) * 0.8):]]\n",
    "y_test = data_y[idx[int(len(idx) * 0.8):]]\n",
    "print(x_train.shape)\n",
    "y_weight = np.unique(y_train, return_counts=True)[1]\n",
    "print(y_weight)\n",
    "print(x_test.shape)\n",
    "print(np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm_problem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ce759214da91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_problem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misKernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-t 1 -c 4 -w0 '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' -w1 '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mp_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svm_problem' is not defined"
     ]
    }
   ],
   "source": [
    "prob = svm_problem(y_train, x_train, isKernel=True)\n",
    "param = svm_parameter('-t 1 -c 4 -w0 ' + str(y_weight[1]) + ' -w1 ' + str(y_weight[0]))\n",
    "m = svm_train(prob, param)\n",
    "p_label, p_acc, p_val = svm_predict(y_test, x_test, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preformance_vacab = []\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    x_v_train = x_train[:, 11*i:11*i+11]\n",
    "    x_v_test = x_test[:, 11*i:11*i+11]\n",
    "    # x_v_train = np.append(x_train[:, 11*i:11*i+11], np.array([x_train[:, -1]]).T, axis=1)\n",
    "    # x_v_test = np.append(x_test[:, 11*i:11*i+11], np.array([x_test[:, -1]]).T, axis=1)\n",
    "    # prob = svm_problem(y_train, x_train[:, 11*i:11*i+11], isKernel=True)\n",
    "    # param = svm_parameter('-t 1 -c 10 -w0 ' + str(y_weight[1]) + ' -w1 ' + str(y_weight[0]))\n",
    "    # m = svm_train(prob, param)\n",
    "    # p_label, p_acc, p_val = svm_predict(y_test, x_test[:, 11*i:11*i+11], m)\n",
    "    # f1 = f1_score(y_test, p_label, average=None)\n",
    "    # print(f1)\n",
    "    clf = svm.SVC(kernel='rbf', C=1, class_weight='balanced').fit(x_v_train, y_train)\n",
    "    y_pred = clf.predict(x_v_test)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "    preformance_vacab.append([i, f1_macro, f1[0], f1[1]])\n",
    "    # print(\"score:\", clf.score(x_test[:, 11*i:11*i+11], y_test))\n",
    "preformance_vacab = np.array(preformance_vacab)\n",
    "preformance_vacab = preformance_vacab[np.argsort(preformance_vacab[:, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.        ,  0.44893685,  0.17283951,  0.7250342 ],\n",
       "       [11.        ,  0.44952274,  0.16806723,  0.73097826],\n",
       "       [13.        ,  0.45393055,  0.15454545,  0.75331565],\n",
       "       [ 1.        ,  0.48629033,  0.1761658 ,  0.79641485],\n",
       "       [ 7.        ,  0.50354369,  0.16      ,  0.84708738]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preformance_vacab[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.20, shuffle=True, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9194444444444444\n",
      "0.8388888888888889\n",
      "0.6777777777777778\n",
      "0.35555555555555557\n",
      "Cytosolic    1800\n",
      "Nucleus      1800\n",
      "Name: loc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "value_counts = train_df[\"loc\"].value_counts()\n",
    "\n",
    "for i in range(len(value_counts)):\n",
    "    while (max(value_counts) - value_counts[i]) / max(value_counts) > .1:\n",
    "        print((max(value_counts) - value_counts[i]) / max(value_counts))\n",
    "        train_df = train_df.append(train_df[train_df[\"loc\"] == value_counts.index[i]][:(max(value_counts) - value_counts[i])])\n",
    "        value_counts = train_df[\"loc\"].value_counts()\n",
    "\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "col = data_df.columns\n",
    "cyt_df = data_df[data_df[\"loc\"] == \"Cytosolic\"]\n",
    "cyt_l = cyt_df.loc[:, [\"len\"]].to_numpy()\n",
    "cyt_l = np.ones_like(cyt_l)\n",
    "\n",
    "len_v = int((len(col)-2) / 11)\n",
    "\n",
    "for i in range(len_v):\n",
    "    for z, l in zip(cyt_df.loc[:, col[11*i:11*i+11]].to_numpy(), cyt_l):\n",
    "        t = np.arange(0, l, 0.01)\n",
    "        p = np.poly1d(z)\n",
    "\n",
    "        plt.plot(t, p(t), alpha=1, color=\"#1f77b4\")\n",
    "        break\n",
    "        \n",
    "    # plt.xlim(0, np.average(cyt_l))\n",
    "    plt.title(vocab[i])\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30459667 0.30563531 0.30480713 0.30405863 0.30499612 0.30685001\n",
      " 0.30871487 0.3108631  0.31331082 0.31590429]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nuc_df = data_df[data_df[\"loc\"] == \"Nucleus\"]\n",
    "nuc_l = nuc_df.loc[:, [\"len\"]].to_numpy()\n",
    "nuc_l = np.ones_like(nuc_l)\n",
    "len_v = int((len(col)-2) / 11)\n",
    "\n",
    "for i in range(len_v):\n",
    "    for z, l in zip(nuc_df.loc[:, col[11*i:11*i+11]].to_numpy(), nuc_l):\n",
    "        t = np.arange(0, l, 0.01)\n",
    "        p = np.poly1d(z)\n",
    "\n",
    "        plt.plot(t, p(t), alpha=0.1, color=\"#1f77b4\")\n",
    "        t = np.linspace(0, 1.0, num=10)\n",
    "        plt.scatter(t, p(t))\n",
    "        print(p(t))\n",
    "        break\n",
    "    # plt.xlim(0, 1)\n",
    "    plt.title(vocab[i])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2Layers(torch.nn.Module):\n",
    "    def __init__(self, ninp: int, nhid: int, ntoken: int, dropout: float=0.0):\n",
    "        \n",
    "        super(NN2Layers, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.nn1 = nn.Linear(ninp, nhid)\n",
    "        self.nn2 = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.ntoken = ntoken\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.nn1.bias.data.zero_()\n",
    "        self.nn1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.nn2.bias.data.zero_()\n",
    "        self.nn2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "\n",
    "        output = self.nn1(x)\n",
    "        output = self.drop(output)\n",
    "        output = self.nn2(output)\n",
    "\n",
    "        return output.softmax(dim=1)\n",
    " \n",
    "    def predict(self, x: torch.tensor):\n",
    "        # \"\"\"預測並輸出機率大的類別\n",
    "\n",
    "        # Args:\n",
    "        #     x (torch.tensor): 詞 tensor。如果batch_first=True，input shape為（批次，序列），否則（序列，批次）。\n",
    "\n",
    "        # Returns:\n",
    "        #     [torch.tensor]: shape 與 x 一樣，但是序列為類別序列。\n",
    "        # \"\"\"\n",
    "        output = self.forward(x)\n",
    "        _, output = torch.max(output, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92699229 0.07300771]\n"
     ]
    }
   ],
   "source": [
    "x = x_train\n",
    "y = y_train\n",
    "# y = np.where(y == \"Cytosolic\" , 1, 0)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "y_weight = np.unique(y, return_counts=True)[1] / len(y)\n",
    "y_weight = np.abs(y_weight - 1)\n",
    "print(y_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoken = 2\n",
    "\n",
    "ninp = x_train.shape[1]\n",
    "nhid = 512\n",
    "\n",
    "model = NN2Layers(ninp, nhid, ntoken)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(y_weight).to(torch.float))\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-1)\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 500\n",
    "\n",
    "tensor_x = torch.tensor(x_train).to(torch.float)\n",
    "tensor_y = torch.tensor(y_train).to(torch.long)\n",
    "\n",
    "test_x = torch.tensor(x_test).to(torch.float)\n",
    "test_y = torch.tensor(y_test).to(torch.long)\n",
    "\n",
    "# dataset = Data.TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "# # train_set, valid_set = Data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# loader = Data.DataLoader(\n",
    "#     dataset = dataset,\n",
    "#     batch_size = batch_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_fn):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def train(self, x, y, epochs = 2, batch_size = 1, epoch_print = True):\n",
    "        train_dataset = Data.TensorDataset(x, y)\n",
    "        loader = Data.DataLoader(\n",
    "            dataset = train_dataset,\n",
    "            batch_size = batch_size,\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        step_size = len(loader)\n",
    "        for epoch in range(epochs):\n",
    "            for step, (batch_x, batch_y) in enumerate(loader):\n",
    "                step_time = time.time()\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                pred_y = self.model(batch_x)\n",
    "                loss = self.loss_fn(pred_y, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # print('Epoch: %i | Step: %i/%i | Loss: %.2f | time: %.2f s' % (epoch, step, step_size, loss, time.time() - step_time))\n",
    "            if epoch_print:\n",
    "                print('Epoch: %i | Loss: %.2f | time: %.2f s' % (epoch, loss, time.time() - step_time))\n",
    "        print('All Time: %.2f s | Loss: %.2f' % (time.time() - start_time, loss))\n",
    "    \n",
    "    def test(self, x, y):\n",
    "        y_pred = self.model.predict(x)\n",
    "        \n",
    "        one_hot_y = np.eye(self.model.ntoken)[y]\n",
    "        one_hot_y_pred = np.eye(self.model.ntoken)[y_pred]\n",
    "        token_acc_array = []\n",
    "        for i in range(self.model.ntoken):\n",
    "            y_token = torch.tensor(one_hot_y[:, i])\n",
    "            y_pred_token = torch.tensor(one_hot_y_pred[:, i])\n",
    "            \n",
    "            tp = (y_token * y_pred_token).sum(dim=0).to(torch.float32)\n",
    "            tn = ((1 - y_token) * (1 - y_pred_token)).sum(dim=0).to(torch.float32)\n",
    "            fp = ((1 - y_token) * y_pred_token).sum(dim=0).to(torch.float32)\n",
    "            fn = (y_token * (1 - y_pred_token)).sum(dim=0).to(torch.float32)\n",
    "            precision = tp / (tp + fp)\n",
    "            rec = tp / (tp + fn)\n",
    "            f1 = 2 * rec * precision / (rec + precision)\n",
    "            token_acc_array.append(f1)\n",
    "        acc = (y_pred == y).float().sum() / len(y)\n",
    "        token_acc_array = torch.tensor(token_acc_array)\n",
    "        return acc, token_acc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, optimizer, loss_fn)\n",
    "trainer.train(tensor_x, tensor_y, epochs, batch_size, epoch_print=False)\n",
    "trainer.test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Time: 43.67 s | Loss: 0.69\n",
      "All Time: 42.74 s | Loss: 0.69\n",
      "All Time: 43.57 s | Loss: 0.69\n",
      "All Time: 44.38 s | Loss: 0.69\n",
      "All Time: 44.36 s | Loss: 0.69\n",
      "All Time: 43.77 s | Loss: 0.69\n",
      "All Time: 43.93 s | Loss: 0.69\n",
      "All Time: 43.48 s | Loss: 0.69\n",
      "All Time: 43.79 s | Loss: 0.69\n",
      "All Time: 44.10 s | Loss: 0.69\n",
      "All Time: 43.94 s | Loss: 0.70\n",
      "All Time: 43.93 s | Loss: 0.69\n",
      "All Time: 43.86 s | Loss: 0.69\n",
      "All Time: 44.28 s | Loss: 0.69\n",
      "All Time: 44.88 s | Loss: 0.69\n",
      "All Time: 45.26 s | Loss: 0.69\n"
     ]
    }
   ],
   "source": [
    "preformance_vacab = []\n",
    "for i in range(len(vocab)):\n",
    "    model = NN2Layers(11, nhid, ntoken)\n",
    "    trainer = Trainer(model, optimizer, loss_fn)\n",
    "    trainer.train(tensor_x[:, 11*i:11*i+11], tensor_y, epochs, batch_size, epoch_print=False)\n",
    "    acc, token_acc_array = trainer.test(test_x[:10, 11*i:11*i+11], test_y[:10])\n",
    "    \n",
    "    preformance_vacab.append(np.append(np.array([i, acc, torch.min(token_acc_array)]), token_acc_array))\n",
    "preformance_vacab = np.array(preformance_vacab)\n",
    "preformance_vacab = preformance_vacab[np.argsort(preformance_vacab[:, 2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.1       ,         nan,  0.18181819,         nan],\n",
       "       [ 1.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [ 2.        ,  0.1       ,         nan,  0.18181819,         nan],\n",
       "       [ 3.        ,  0.1       ,         nan,  0.18181819,         nan],\n",
       "       [ 4.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [ 5.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [ 6.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [ 7.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [ 8.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [ 9.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [10.        ,  0.1       ,         nan,  0.18181819,         nan],\n",
       "       [11.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [12.        ,  0.1       ,         nan,  0.18181819,         nan],\n",
       "       [13.        ,  0.89999998,         nan,         nan,  0.94736838],\n",
       "       [14.        ,  0.1       ,         nan,  0.18181819,         nan],\n",
       "       [15.        ,  0.89999998,         nan,         nan,  0.94736838]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preformance_vacab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.67 | Accuracy: 91.57 | time: 0.01 s\n",
      "Epoch: 1 | Loss: 0.65 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 2 | Loss: 0.62 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 3 | Loss: 0.59 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 4 | Loss: 0.56 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 5 | Loss: 0.52 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 6 | Loss: 0.49 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 7 | Loss: 0.46 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 8 | Loss: 0.44 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 9 | Loss: 0.42 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 10 | Loss: 0.40 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 11 | Loss: 0.39 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 12 | Loss: 0.38 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 13 | Loss: 0.37 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 14 | Loss: 0.36 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 15 | Loss: 0.35 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 16 | Loss: 0.35 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 17 | Loss: 0.34 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 18 | Loss: 0.34 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 19 | Loss: 0.34 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 20 | Loss: 0.33 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 21 | Loss: 0.33 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 22 | Loss: 0.33 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 23 | Loss: 0.33 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 24 | Loss: 0.33 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 25 | Loss: 0.33 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 26 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 27 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 28 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 29 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 30 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 31 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 32 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 33 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 34 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 35 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 36 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 37 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 38 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 39 | Loss: 0.32 | Accuracy: 92.19 | time: 0.01 s\n",
      "Epoch: 40 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 41 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 42 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 43 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 44 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 45 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 46 | Loss: 0.32 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 47 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 48 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 49 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 50 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 51 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 52 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 53 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 54 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 55 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 56 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 57 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 58 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 59 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 60 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 61 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 62 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 63 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 64 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 65 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 66 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 67 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 68 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 69 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 70 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 71 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 72 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 73 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 74 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 75 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 76 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 77 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 78 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 79 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 80 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 81 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 82 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 83 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 84 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 85 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 86 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 87 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 88 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 89 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 90 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 91 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 92 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 93 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 94 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 95 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 96 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 97 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 98 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 99 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 100 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 101 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 102 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 103 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 104 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 105 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 106 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 107 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 108 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 109 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 110 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 111 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 112 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 113 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 114 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 115 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 116 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 117 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 118 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 119 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 120 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 121 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 122 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 123 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 124 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 125 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 126 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 127 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 128 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 129 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 130 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 131 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 132 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 133 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 134 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 135 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 136 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 137 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 138 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 139 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 140 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 141 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 142 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 143 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 144 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 145 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 146 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 147 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 148 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 149 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 150 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 151 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 152 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 153 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 154 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 155 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 156 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 157 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 158 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 159 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 160 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 161 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 162 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 163 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 164 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 165 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 166 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 167 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 168 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 169 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 170 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 171 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 172 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 173 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 174 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 175 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 176 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 177 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 178 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 179 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 180 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 181 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 182 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 183 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 184 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 185 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 186 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 187 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 188 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 189 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 190 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 191 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 192 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 193 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 194 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 195 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 196 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 197 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 198 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 199 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 200 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 201 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 202 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 203 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 204 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 205 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 206 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 207 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 208 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 209 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 210 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 211 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 212 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 213 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 214 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 215 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 216 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 217 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 218 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 219 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 220 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 221 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 222 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 223 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 224 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 225 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 226 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 227 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 228 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 229 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 230 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 231 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 232 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 233 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 234 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 235 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 236 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 237 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 238 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 239 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 240 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 241 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 242 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 243 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 244 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 245 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 246 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 247 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 248 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 249 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 250 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 251 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 252 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 253 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 254 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 255 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 256 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 257 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 258 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 259 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 260 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 261 | Loss: 0.31 | Accuracy: 92.19 | time: 0.01 s\n",
      "Epoch: 262 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 263 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 264 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 265 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 266 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 267 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 268 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 269 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 270 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 271 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 272 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 273 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 274 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 275 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 276 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 277 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 278 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 279 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 280 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 281 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 282 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 283 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 284 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 285 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 286 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 287 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 288 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 289 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 290 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 291 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 292 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 293 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 294 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 295 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 296 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 297 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 298 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 299 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 300 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 301 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 302 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 303 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 304 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 305 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 306 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 307 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 308 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 309 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 310 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 311 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 312 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 313 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 314 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 315 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 316 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 317 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 318 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 319 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 320 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 321 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 322 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 323 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 324 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 325 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 326 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 327 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 328 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 329 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 330 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 331 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 332 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 333 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 334 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 335 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 336 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 337 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 338 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 339 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 340 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 341 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 342 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 343 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 344 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 345 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 346 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 347 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 348 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 349 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 350 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 351 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 352 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 353 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 354 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 355 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 356 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 357 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 358 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 359 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 360 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 361 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 362 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 363 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 364 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 365 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 366 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 367 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 368 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 369 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 370 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 371 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 372 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 373 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 374 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 375 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 376 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 377 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 378 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 379 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 380 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 381 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 382 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 383 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 384 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 385 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 386 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 387 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 388 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 389 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 390 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 391 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 392 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 393 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 394 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 395 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 396 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 397 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 398 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 399 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 400 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 401 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 402 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 403 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 404 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 405 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 406 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 407 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 408 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 409 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 410 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 411 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 412 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 413 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 414 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 415 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 416 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 417 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 418 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 419 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 420 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 421 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 422 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 423 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 424 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 425 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 426 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 427 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 428 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 429 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 430 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 431 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 432 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 433 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 434 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 435 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 436 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 437 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 438 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 439 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 440 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 441 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 442 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 443 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 444 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 445 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 446 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 447 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 448 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 449 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 450 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 451 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 452 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 453 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 454 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 455 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 456 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 457 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 458 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 459 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 460 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 461 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 462 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 463 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 464 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 465 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 466 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 467 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 468 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 469 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 470 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 471 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 472 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 473 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 474 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 475 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 476 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 477 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 478 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 479 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 480 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 481 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 482 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 483 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 484 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 485 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 486 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 487 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 488 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 489 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 490 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 491 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 492 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 493 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 494 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 495 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 496 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 497 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 498 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 499 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 500 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 501 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 502 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 503 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 504 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 505 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 506 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 507 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 508 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 509 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 510 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 511 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 512 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 513 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 514 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 515 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 516 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 517 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 518 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 519 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 520 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 521 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 522 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 523 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 524 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 525 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 526 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 527 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 528 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 529 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 530 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 531 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 532 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 533 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 534 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 535 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 536 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 537 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 538 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 539 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 540 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 541 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 542 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 543 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 544 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 545 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 546 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 547 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 548 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 549 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 550 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 551 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 552 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 553 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 554 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 555 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 556 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 557 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 558 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 559 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 560 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 561 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 562 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 563 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 564 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 565 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 566 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 567 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 568 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 569 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 570 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 571 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 572 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 573 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 574 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 575 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 576 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 577 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 578 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 579 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 580 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 581 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 582 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 583 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 584 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 585 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 586 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 587 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 588 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 589 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 590 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 591 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 592 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 593 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 594 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 595 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 596 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 597 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 598 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 599 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 600 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 601 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 602 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 603 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 604 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 605 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 606 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 607 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 608 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 609 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 610 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 611 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 612 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 613 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 614 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 615 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 616 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 617 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 618 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 619 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 620 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 621 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 622 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 623 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 624 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 625 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 626 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 627 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 628 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 629 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 630 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 631 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 632 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 633 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 634 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 635 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 636 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 637 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 638 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 639 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 640 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 641 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 642 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 643 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 644 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 645 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 646 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 647 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 648 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 649 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 650 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 651 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 652 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 653 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 654 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 655 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 656 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 657 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 658 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 659 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 660 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 661 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 662 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 663 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 664 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 665 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 666 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 667 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 668 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 669 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 670 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 671 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 672 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 673 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 674 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 675 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 676 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 677 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 678 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 679 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 680 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 681 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 682 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 683 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 684 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 685 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 686 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 687 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 688 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 689 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 690 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 691 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 692 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 693 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 694 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 695 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 696 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 697 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 698 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 699 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 700 | Loss: 0.31 | Accuracy: 92.19 | time: 0.01 s\n",
      "Epoch: 701 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 702 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 703 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 704 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 705 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 706 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 707 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 708 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 709 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 710 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 711 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 712 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 713 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 714 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 715 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 716 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 717 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 718 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 719 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 720 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 721 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 722 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 723 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 724 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 725 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 726 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 727 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 728 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 729 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 730 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 731 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 732 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 733 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 734 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 735 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 736 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 737 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 738 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 739 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 740 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 741 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 742 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 743 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 744 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 745 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 746 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 747 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 748 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 749 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 750 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 751 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 752 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 753 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 754 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 755 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 756 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 757 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 758 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 759 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 760 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 761 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 762 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 763 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 764 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 765 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 766 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 767 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 768 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 769 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 770 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 771 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 772 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 773 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 774 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 775 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 776 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 777 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 778 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 779 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 780 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 781 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 782 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 783 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 784 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 785 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 786 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 787 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 788 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 789 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 790 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 791 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 792 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 793 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 794 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 795 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 796 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 797 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 798 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 799 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 800 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 801 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 802 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 803 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 804 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 805 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 806 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 807 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 808 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 809 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 810 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 811 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 812 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 813 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 814 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 815 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 816 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 817 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 818 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 819 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 820 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 821 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 822 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 823 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 824 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 825 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 826 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 827 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 828 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 829 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 830 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 831 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 832 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 833 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 834 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 835 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 836 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 837 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 838 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 839 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 840 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 841 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 842 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 843 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 844 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 845 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 846 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 847 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 848 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 849 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 850 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 851 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 852 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 853 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 854 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 855 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 856 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 857 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 858 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 859 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 860 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 861 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 862 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 863 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 864 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 865 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 866 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 867 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 868 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 869 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 870 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 871 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 872 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 873 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 874 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 875 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 876 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 877 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 878 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 879 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 880 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 881 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 882 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 883 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 884 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 885 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 886 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 887 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 888 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 889 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 890 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 891 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 892 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 893 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 894 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 895 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 896 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 897 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 898 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 899 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 900 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 901 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 902 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 903 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 904 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 905 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 906 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 907 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 908 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 909 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 910 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 911 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 912 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 913 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 914 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 915 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 916 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 917 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 918 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 919 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 920 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 921 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 922 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 923 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 924 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 925 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 926 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 927 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 928 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 929 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 930 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 931 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 932 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 933 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 934 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 935 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 936 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 937 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 938 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 939 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 940 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 941 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 942 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 943 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 944 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 945 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 946 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 947 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 948 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 949 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 950 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 951 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 952 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 953 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 954 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 955 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 956 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 957 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 958 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 959 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 960 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 961 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 962 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 963 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 964 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 965 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 966 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 967 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 968 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 969 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 970 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 971 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 972 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 973 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 974 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 975 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 976 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 977 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 978 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 979 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 980 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 981 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 982 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 983 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 984 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 985 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 986 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 987 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 988 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 989 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 990 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 991 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 992 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 993 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 994 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 995 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 996 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 997 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 998 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 999 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1000 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1001 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1002 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1003 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1004 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1005 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1006 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1007 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1008 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1009 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1010 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1011 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1012 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1013 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1014 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1015 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1016 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1017 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1018 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1019 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1020 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1021 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1022 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1023 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1024 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1025 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1026 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1027 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1028 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1029 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1030 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1031 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1032 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1033 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1034 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1035 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1036 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1037 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1038 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1039 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1040 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1041 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1042 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1043 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1044 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1045 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1046 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1047 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1048 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1049 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1050 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1051 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1052 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1053 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1054 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1055 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1056 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1057 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1058 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1059 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1060 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1061 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1062 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1063 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1064 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1065 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1066 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1067 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1068 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1069 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1070 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1071 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1072 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1073 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1074 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1075 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1076 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1077 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1078 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1079 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1080 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1081 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1082 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1083 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1084 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1085 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1086 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1087 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1088 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1089 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1090 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1091 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1092 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1093 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1094 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1095 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1096 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1097 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1098 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1099 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1100 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1101 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1102 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1103 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1104 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1105 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1106 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1107 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1108 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1109 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1110 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1111 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1112 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1113 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1114 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1115 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1116 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1117 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1118 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1119 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1120 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1121 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1122 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1123 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1124 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1125 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1126 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1127 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1128 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1129 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1130 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n",
      "Epoch: 1131 | Loss: 0.31 | Accuracy: 92.19 | time: 0.00 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-ba3fc64bd2ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# print('Epoch: %i | Step: %i/%i | Loss: %.2f | time: %.2f s' % (epoch, step, step_size, loss, time.time() - step_time))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jand/.conda/envs/jand_venv/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jand/.conda/envs/jand_venv/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jand/.conda/envs/jand_venv/lib/python3.6/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                     eps=group['eps'])\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jand/.conda/envs/jand_venv/lib/python3.6/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "step_size = len(loader)\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        step_time = time.time()\n",
    "        model.zero_grad()\n",
    "        pred = model(batch_x)\n",
    "        loss = loss_fn(pred, batch_y)\n",
    "        # print('Epoch: %i | Step: %i/%i | Loss: %.2f | time: %.2f s' % (epoch, step, step_size, loss, time.time() - step_time))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct += (torch.argmax(pred, dim=1) == batch_y).float().sum()\n",
    "\n",
    "    accuracy = 100 * correct / len(loader.dataset)\n",
    "    print('Epoch: %i | Loss: %.2f | Accuracy: %.2f | time: %.2f s' % (epoch, loss, accuracy, time.time() - step_time))\n",
    "\n",
    "print('all time : ', time.time() - start_time,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7311, 0.2689],\n",
      "        [0.7310, 0.2690],\n",
      "        [0.7311, 0.2689],\n",
      "        [0.7311, 0.2689],\n",
      "        [0.7310, 0.2690],\n",
      "        [0.7305, 0.2695],\n",
      "        [0.7311, 0.2689],\n",
      "        [0.7311, 0.2689],\n",
      "        [0.7298, 0.2702],\n",
      "        [0.7311, 0.2689]])\n",
      "tensor([[1.0000e+00, 2.6870e-09],\n",
      "        [9.9980e-01, 2.0265e-04],\n",
      "        [1.0000e+00, 1.9828e-07],\n",
      "        [1.0000e+00, 2.3776e-15],\n",
      "        [9.9996e-01, 4.4885e-05],\n",
      "        [9.9870e-01, 1.3012e-03],\n",
      "        [1.0000e+00, 1.5889e-10],\n",
      "        [9.9999e-01, 1.0258e-05],\n",
      "        [9.9675e-01, 3.2475e-03],\n",
      "        [1.0000e+00, 8.9727e-10]])\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "def valid(vaild_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            pred = model(batch_x)\n",
    "            print(pred.softmax(dim=1))\n",
    "            print(pred)\n",
    "            print(batch_y)\n",
    "            break\n",
    "\n",
    "valid_loader = Data.DataLoader(\n",
    "    dataset = valid_set,\n",
    "    batch_size = 5,\n",
    ")\n",
    "valid(valid_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
