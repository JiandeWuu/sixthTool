{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from os.path import isfile\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from libsvm.svmutil import svm_problem\n",
    "from libsvm.svmutil import svm_parameter\n",
    "from libsvm.svmutil import svm_train\n",
    "from libsvm.svmutil import svm_predict\n",
    "from libsvm.svmutil import evaluations\n",
    "\n",
    "from Function import svm_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"data/merge_data/output/\"\n",
    "onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "\n",
    "perf_list = []\n",
    "\n",
    "for file_name in onlyfiles:\n",
    "    if \"__esvm_e512_s10_f10_p10.json\" in file_name and not \".dvc\" in file_name:\n",
    "        file_str = file_name[:-len(\"__esvm_e512_s10_f10_p10.json\")]\n",
    "        \n",
    "        f = open(dir_path + file_name)\n",
    "        perf = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        perf_list.append([file_str, np.load(\"data/merge_data/train/1230_train__\" + file_str + \".npy\").shape[1],\n",
    "                          perf['avg Accy'], perf['std Accy'],\n",
    "                          perf['avg Recall'], perf['std Recall'],\n",
    "                          perf['avg Prec'], perf['std Prec'],\n",
    "                          perf['avg Spec'], perf['std Spec'],\n",
    "                          perf['avg Npv'], perf['std Npv'],\n",
    "                          perf['avg F1sc'], perf['std F1sc'],\n",
    "                          perf['avg AUROC'], perf['std AUROC'],\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(perf_list, columns=[\"esvm_s10\", \"nums\", \n",
    "                                 \"avg Accy\", \"std Accy\",\n",
    "                                 \"avg Recall\", \"std Recall\",\n",
    "                                 \"avg Prec\", \"std Prec\",\n",
    "                                 \"avg Spec\", \"std Spec\",\n",
    "                                 \"avg Npv\", \"std Npv\",\n",
    "                                 \"avg F1sc\", \"std F1sc\",\n",
    "                                 \"avg AUROC\", \"std AUROC\"\n",
    "                                 ]).sort_values(\"esvm_s10\").to_csv(\"data/merge_data/output/esvm_e512_s10_f10_p10__pref.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2264, 1008)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"data/linear_features/point/benchmark/train/k234p10nor2n3_train.npy\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file_str',\n",
       " 0.7062645551932929,\n",
       " 0.02728313642700328,\n",
       " 0.724779861183052,\n",
       " 0.027373539164752204,\n",
       " 0.9191811524505985,\n",
       " 0.013461697940586836,\n",
       " 0.585483870967742,\n",
       " 0.0729476979210487,\n",
       " 0.8102467384324458,\n",
       " 0.019711847903634495,\n",
       " 0.6936555352450462,\n",
       " 0.04558923440965074]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"data/merge_data/output/Kmer_k3__DACC_lag2__SC-PseDNC-General_all_index_svmf10p4e512.json\")\n",
    "perf = json.load(f)\n",
    "f.close()\n",
    "[\"file_str\", \n",
    "        perf['avg Accy'], perf['std Accy'],\n",
    "        perf['avg Recall'], perf['std Recall'],\n",
    "        perf['avg Prec'], perf['std Prec'],\n",
    "        perf['avg Spec'], perf['std Spec'],\n",
    "        perf['avg F1sc'], perf['std F1sc'],\n",
    "        perf['avg AUROC'], perf['std AUROC'],\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import average\n",
    "\n",
    "\n",
    "dir_path = \"data/linear_features/point/benchmark/output/\"\n",
    "onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "\n",
    "perf_list = []\n",
    "\n",
    "for file_name in onlyfiles:\n",
    "    if \"_svmf10p4e512.json\" in file_name and not \".dvc\" in file_name and not \"train\" in file_name:\n",
    "        \n",
    "        file_str = file_name[:-5]\n",
    "        f = open(dir_path + file_name)\n",
    "        perf = json.load(f)\n",
    "        f.close()\n",
    "        perf_list.append([file_str, np.load(\"data/linear_features/point/benchmark/train/\" + file_name[:-18] + \"_train.npy\").shape[1],\n",
    "                          perf['avg Accy'], perf['std Accy'],\n",
    "                          perf['avg Recall'], perf['std Recall'],\n",
    "                          perf['avg Prec'], perf['std Prec'],\n",
    "                          perf['avg Spec'], perf['std Spec'],\n",
    "                          perf['avg F1sc'], perf['std F1sc'],\n",
    "                          perf['avg AUROC'], perf['std AUROC'],\n",
    "                          ])\n",
    "        # npv_array = []\n",
    "        # for i in range(len(perf['confusion matrix'])):\n",
    "        #     tn, fp, fn, tp = np.array(perf['confusion matrix'][i][1]).flatten()\n",
    "        #     acc = (tn + tp) / (tn + fp + fn + tp)\n",
    "        #     recall = tp / (fn + tp)\n",
    "        #     prec = (tp / (fp + tp))\n",
    "        #     spec = (tn / (tn + fp))\n",
    "        #     npv = (tn / (tn + fn))\n",
    "        #     f1sc = (2 * (tp / (fn + tp)) * (tp / (fp + tp)) / ((tp / (fn + tp)) + (tp / (fp + tp))))\n",
    "        #     npv_array.append(npv)\n",
    "        # print(npv_array)\n",
    "        # print(sum(npv_array)/ len(npv_array), np.std(npv_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(perf_list, columns=['file name', \"num\", 'avg Accy', 'std Accy', 'avg Recall', 'std Recall', 'avg Prec', 'std Prec', 'avg Spec', 'std Spec', 'avg F1sc', 'std F1sc', 'avg AUROC', 'std AUROC']).sort_values('file name').to_csv(dir_path + \"svmf10p4e512_pref_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jand_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "681206c3b8dadcd0ad9db9cda94afb40592645b096d359128b6de26b00c40796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
