{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.load(\"data/k_mers/k4_datax.npy\")\n",
    "data_y = np.load(\"data/k_mers/k4_datay.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2Layers(torch.nn.Module):\n",
    "    def __init__(self, ninp: int, nhid: int, ntoken: int, dropout: float=0.0):\n",
    "        \n",
    "        super(NN2Layers, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.nn1 = nn.Linear(ninp, nhid)\n",
    "        self.nn2 = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.ntoken = ntoken\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.nn1.bias.data.zero_()\n",
    "        self.nn1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.nn2.bias.data.zero_()\n",
    "        self.nn2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "\n",
    "        output = self.nn1(x)\n",
    "        output = self.drop(output)\n",
    "        output = self.nn2(output)\n",
    "\n",
    "        return output.softmax(dim=1)\n",
    " \n",
    "    def predict(self, x: torch.tensor):\n",
    "        # \"\"\"預測並輸出機率大的類別\n",
    "\n",
    "        # Args:\n",
    "        #     x (torch.tensor): 詞 tensor。如果batch_first=True，input shape為（批次，序列），否則（序列，批次）。\n",
    "\n",
    "        # Returns:\n",
    "        #     [torch.tensor]: shape 與 x 一樣，但是序列為類別序列。\n",
    "        # \"\"\"\n",
    "        output = self.forward(x)\n",
    "        _, output = torch.max(output, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_fn):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def train(self, x, y, valid_x, valid_y, epochs = 2, batch_size = 1, epoch_print = True, sampler = None):\n",
    "        # Early stopping\n",
    "        the_last_loss = 100\n",
    "        patience = 2\n",
    "        trigger_times = 0\n",
    "        \n",
    "        train_dataset = Data.TensorDataset(x, y)\n",
    "        loader = Data.DataLoader(\n",
    "            dataset = train_dataset,\n",
    "            batch_size = batch_size,\n",
    "            sampler = sampler,\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        step_size = len(loader)\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_time = time.time()\n",
    "            for step, (batch_x, batch_y) in enumerate(loader):\n",
    "                step_time = time.time()\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                pred_y = self.model(batch_x)\n",
    "                loss = self.loss_fn(pred_y, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                loss_history.append(loss.item())\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            # Early stopping\n",
    "            the_current_loss = self.validation(valid_x, valid_y, batch_size=batch_size)\n",
    "            # print('The current loss:', the_current_loss)\n",
    "            \n",
    "            if the_current_loss >= the_last_loss:\n",
    "                trigger_times += 1\n",
    "                # print('trigger times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping!\\nStart to test process.')\n",
    "                    break\n",
    "            else:\n",
    "                # print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "            \n",
    "            if epoch_print:\n",
    "                print('Epoch: %i | Loss: %.2f | time: %.2f s' % (epoch, the_current_loss, time.time() - epoch_time))\n",
    "            \n",
    "            the_last_loss = the_current_loss\n",
    "            \n",
    "        print('All Time: %.2f s | Loss: %.2f' % (time.time() - start_time, the_current_loss))\n",
    "    \n",
    "    def validation(self, valid_x, valid_y, batch_size = 1):\n",
    "        train_dataset = Data.TensorDataset(valid_x, valid_y)\n",
    "        valid_loader = Data.DataLoader(\n",
    "            dataset = train_dataset,\n",
    "            batch_size = batch_size,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        loss_total = 0\n",
    "\n",
    "        # Test validation data\n",
    "        with torch.no_grad():\n",
    "            for step, (batch_x, batch_y) in enumerate(valid_loader):\n",
    "\n",
    "                outputs = model(batch_x)\n",
    "                loss = self.loss_fn(outputs, batch_y)\n",
    "                loss_total += loss.item()\n",
    "\n",
    "        return loss_total / len(valid_loader)\n",
    "\n",
    "    def test(self, x, y):\n",
    "        y_pred = self.model.predict(x)\n",
    "        \n",
    "        one_hot_y = np.eye(self.model.ntoken)[y]\n",
    "        one_hot_y_pred = np.eye(self.model.ntoken)[y_pred]\n",
    "        token_acc_array = []\n",
    "        for i in range(self.model.ntoken):\n",
    "            y_token = torch.tensor(one_hot_y[:, i])\n",
    "            y_pred_token = torch.tensor(one_hot_y_pred[:, i])\n",
    "            \n",
    "            tp = (y_token * y_pred_token).sum(dim=0).to(torch.float32)\n",
    "            tn = ((1 - y_token) * (1 - y_pred_token)).sum(dim=0).to(torch.float32)\n",
    "            fp = ((1 - y_token) * y_pred_token).sum(dim=0).to(torch.float32)\n",
    "            fn = (y_token * (1 - y_pred_token)).sum(dim=0).to(torch.float32)\n",
    "            precision = tp / (tp + fp)\n",
    "            rec = tp / (tp + fn)\n",
    "            f1 = 2 * rec * precision / (rec + precision)\n",
    "            token_acc_array.append(f1)\n",
    "        acc = (y_pred == y).float().sum() / len(y)\n",
    "        token_acc_array = torch.tensor(token_acc_array)\n",
    "        return acc, token_acc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV(x, y, folder):\n",
    "    unique, count = np.unique(y, return_counts=True)\n",
    "    cv_x = []\n",
    "    cv_y = []\n",
    "    for u in unique:\n",
    "        u_x = x[y == u]\n",
    "        u_y = y[y == u]\n",
    "        arr = np.arange(len(u_x))\n",
    "        np.random.shuffle(arr)\n",
    "        u_x = u_x[arr]\n",
    "        u_y = u_y[arr]\n",
    "        \n",
    "        linspace = np.linspace(0, len(u_x), folder + 1, dtype=int)\n",
    "        \n",
    "        for i in range(folder):\n",
    "            if unique[0] == u:\n",
    "                cv_x.append(u_x[linspace[i]:linspace[i+1]])\n",
    "                cv_y.append(u_y[linspace[i]:linspace[i+1]])\n",
    "            else:\n",
    "                cv_x[i] = np.append(cv_x[i], u_x[linspace[i]:linspace[i+1]], axis=0)\n",
    "                cv_y[i] = np.append(cv_y[i], u_y[linspace[i]:linspace[i+1]], axis=0)\n",
    "    return cv_x, cv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (3000, 20)\n",
      "> Class=0 : 171/3000 (5.7%)\n",
      "> Class=1 : 2829/3000 (94.3%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "# generate 2 class dataset\n",
    "x, y = make_classification(n_samples=3000, n_classes=2, weights=[0.05, 0.95], random_state=0)\n",
    "# summarize dataset\n",
    "print(\"x\", x.shape)\n",
    "classes = np.unique(y)\n",
    "total = len(y)\n",
    "for c in classes:\n",
    "\tn_examples = len(y[y==c])\n",
    "\tpercent = n_examples / total * 100\n",
    "\tprint('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 29.52 s | Loss: 0.77\n",
      "tensor(0.0726) tensor([0.1354,    nan])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 44.65 s | Loss: 0.54\n",
      "tensor(0.9243) tensor([   nan, 0.9607])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 66.42 s | Loss: 0.86\n",
      "tensor(0.0757) tensor([0.1407,    nan])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 29.44 s | Loss: 0.77\n",
      "tensor(0.0757) tensor([0.1407,    nan])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 14.57 s | Loss: 0.89\n",
      "tensor(0.0757) tensor([0.1407,    nan])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 40.45 s | Loss: 0.94\n",
      "tensor(0.0757) tensor([0.1407,    nan])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 18.12 s | Loss: 0.93\n",
      "tensor(0.0757) tensor([0.1407,    nan])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 18.28 s | Loss: 0.86\n",
      "tensor(0.0757) tensor([0.1407,    nan])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 18.44 s | Loss: 0.95\n",
      "tensor(0.0757) tensor([0.1407,    nan])\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 18.27 s | Loss: 0.71\n",
      "tensor(0.0754) tensor([0.1402,    nan])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntoken = 2\n",
    "nhid = 512\n",
    "\n",
    "batch_size = 1\n",
    "lr = 3e-1\n",
    "epochs = 50\n",
    "\n",
    "cv_x, cv_y = CV(data_x, data_y, 10)\n",
    "# cv_x, cv_y = CV(x, y, 10)\n",
    "score_array = []\n",
    "for i in range(len(cv_x)):\n",
    "    train_x = None\n",
    "    for j in range(len(cv_x)):\n",
    "        if i == j :\n",
    "            test_x = cv_x[i]\n",
    "            test_y = cv_y[i]\n",
    "        else:\n",
    "            if train_x is None:\n",
    "                train_x = cv_x[i]\n",
    "                train_y = cv_y[i]\n",
    "            else:\n",
    "                train_x = np.append(train_x, cv_x[i], axis=0)\n",
    "                train_y = np.append(train_y, cv_y[i], axis=0)\n",
    "    \n",
    "    ninp = train_x.shape[1]\n",
    "\n",
    "    model = NN2Layers(ninp, nhid, ntoken)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    tensor_x = torch.tensor(train_x).to(torch.float)\n",
    "    tensor_y = torch.tensor(train_y).to(torch.long)\n",
    "\n",
    "    test_x = torch.tensor(test_x).to(torch.float)\n",
    "    test_y = torch.tensor(test_y).to(torch.long)\n",
    "    \n",
    "    weight = 1. / np.unique(train_y, return_counts=True)[1]\n",
    "    samples_weight = np.array([weight[t] for t in tensor_y])\n",
    "\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    \n",
    "    trainer = Trainer(model, optimizer, loss_fn)\n",
    "    trainer.train(tensor_x, tensor_y, test_x, test_y, epochs=epochs, batch_size=batch_size, epoch_print=False, sampler=sampler)\n",
    "    acc, token_acc_array = trainer.test(test_x, test_y)\n",
    "    print(acc, token_acc_array)\n",
    "    score_array.append(sum(token_acc_array) / len(token_acc_array))\n",
    "    \n",
    "sum(score_array) / len(score_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "681206c3b8dadcd0ad9db9cda94afb40592645b096d359128b6de26b00c40796"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('jand_venv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
