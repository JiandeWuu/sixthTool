{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2432, 4, 9) (2432,)\n"
     ]
    }
   ],
   "source": [
    "data_x = np.load(\"data/linear_features/linear/k1p8nor3.npy\")\n",
    "# data_x = np.load(\"data/linear_features/point/k2p10nor2n10.npy\")\n",
    "# data_x = data_x.reshape(data_x.shape[0],-1)\n",
    "\n",
    "data_y = np.load(\"data/linear_features/data_y.npy\")\n",
    "print(data_x.shape, data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.20, shuffle=True, random_state=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2Layers(torch.nn.Module):\n",
    "    def __init__(self, ninp: int, nhid: int, ntoken: int, dropout: float=0.0):\n",
    "        \n",
    "        super(NN2Layers, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.nn1 = nn.Linear(ninp, nhid)\n",
    "        self.nn2 = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.ntoken = ntoken\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.nn1.bias.data.zero_()\n",
    "        self.nn1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.nn2.bias.data.zero_()\n",
    "        self.nn2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "\n",
    "        output = self.nn1(x)\n",
    "        output = self.drop(output)\n",
    "        output = self.nn2(output)\n",
    "\n",
    "        return output.softmax(dim=1)\n",
    " \n",
    "    def predict(self, x: torch.tensor):\n",
    "        # \"\"\"預測並輸出機率大的類別\n",
    "\n",
    "        # Args:\n",
    "        #     x (torch.tensor): 詞 tensor。如果batch_first=True，input shape為（批次，序列），否則（序列，批次）。\n",
    "\n",
    "        # Returns:\n",
    "        #     [torch.tensor]: shape 與 x 一樣，但是序列為類別序列。\n",
    "        # \"\"\"\n",
    "        output = self.forward(x)\n",
    "        _, output = torch.max(output, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, tagset_size):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        output, hn = self.rnn(sentence)\n",
    "        tag_space = self.hidden2tag(output)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)[:, :, -1]\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoken = 2\n",
    "\n",
    "ninp = x_train.shape[2]\n",
    "nhid = 256\n",
    "\n",
    "model = RNNTagger(ninp, nhid, ntoken)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=4e-3, weight_decay=1e-4)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=4e-1)\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 50000\n",
    "\n",
    "tensor_x = torch.tensor(x_train).to(torch.float)\n",
    "tensor_y = torch.tensor(y_train).to(torch.long)\n",
    "\n",
    "test_x = torch.tensor(x_test).to(torch.float)\n",
    "test_y = torch.tensor(y_test).to(torch.long)\n",
    "\n",
    "# dataset = Data.TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "# # train_set, valid_set = Data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# loader = Data.DataLoader(\n",
    "#     dataset = dataset,\n",
    "#     batch_size = batch_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, loss_fn):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def train(self, x, y, valid_x, valid_y, epochs = 2, batch_size = 1, epoch_print = True, sampler = None):\n",
    "        # Early stopping\n",
    "        the_last_loss = 100\n",
    "        patience = 2\n",
    "        trigger_times = 0\n",
    "        \n",
    "        train_dataset = Data.TensorDataset(x, y)\n",
    "        loader = Data.DataLoader(\n",
    "            dataset = train_dataset,\n",
    "            batch_size = batch_size,\n",
    "            sampler = sampler,\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        step_size = len(loader)\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_time = time.time()\n",
    "            for step, (batch_x, batch_y) in enumerate(loader):\n",
    "                step_time = time.time()\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                pred_y = self.model(batch_x)\n",
    "                loss = self.loss_fn(pred_y, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                loss_history.append(loss.item())\n",
    "                epoch_loss += loss.item()\n",
    "                # print('Epoch: %i | Step: %i/%i | Loss: %.2f | time: %.2f s' % (epoch, step, step_size, loss, time.time() - step_time))\n",
    "            \n",
    "            \n",
    "            # Early stopping\n",
    "            the_current_loss = self.validation(valid_x, valid_y, batch_size=batch_size)\n",
    "            # print('The current loss:', the_current_loss)\n",
    "            \n",
    "            if the_current_loss >= the_last_loss:\n",
    "                trigger_times += 1\n",
    "                print('trigger times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping!\\nStart to test process.')\n",
    "                    break\n",
    "            else:\n",
    "                print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "            \n",
    "            if epoch_print:\n",
    "                print('Epoch: %i | Loss: %.2f | time: %.2f s' % (epoch, the_current_loss, time.time() - epoch_time))\n",
    "            \n",
    "            the_last_loss = the_current_loss\n",
    "            \n",
    "        print('All Time: %.2f s | Loss: %.2f' % (time.time() - start_time, the_current_loss))\n",
    "    \n",
    "    def validation(self, valid_x, valid_y, batch_size = 1):\n",
    "        train_dataset = Data.TensorDataset(valid_x, valid_y)\n",
    "        valid_loader = Data.DataLoader(\n",
    "            dataset = train_dataset,\n",
    "            batch_size = batch_size,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        loss_total = 0\n",
    "\n",
    "        # Test validation data\n",
    "        with torch.no_grad():\n",
    "            for step, (batch_x, batch_y) in enumerate(valid_loader):\n",
    "\n",
    "                outputs = model(batch_x)\n",
    "                loss = self.loss_fn(outputs, batch_y)\n",
    "                loss_total += loss.item()\n",
    "\n",
    "        return loss_total / len(valid_loader)\n",
    "\n",
    "    def test(self, x, y):\n",
    "        y_pred = self.model.predict(x)\n",
    "        \n",
    "        one_hot_y = np.eye(self.model.ntoken)[y]\n",
    "        one_hot_y_pred = np.eye(self.model.ntoken)[y_pred]\n",
    "        token_acc_array = []\n",
    "        for i in range(self.model.ntoken):\n",
    "            y_token = torch.tensor(one_hot_y[:, i])\n",
    "            y_pred_token = torch.tensor(one_hot_y_pred[:, i])\n",
    "            \n",
    "            tp = (y_token * y_pred_token).sum(dim=0).to(torch.float32)\n",
    "            tn = ((1 - y_token) * (1 - y_pred_token)).sum(dim=0).to(torch.float32)\n",
    "            fp = ((1 - y_token) * y_pred_token).sum(dim=0).to(torch.float32)\n",
    "            fn = (y_token * (1 - y_pred_token)).sum(dim=0).to(torch.float32)\n",
    "            precision = tp / (tp + fp)\n",
    "            rec = tp / (tp + fn)\n",
    "            f1 = 2 * rec * precision / (rec + precision)\n",
    "            token_acc_array.append(f1)\n",
    "        acc = (y_pred == y).float().sum() / len(y)\n",
    "        token_acc_array = torch.tensor(token_acc_array)\n",
    "        return acc, token_acc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 1. / np.unique(y_train, return_counts=True)[1]\n",
    "samples_weight = np.array([weight[t] for t in tensor_y])\n",
    "\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weigth = samples_weight.double()\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigger times: 0\n",
      "Epoch: 0 | Loss: 1.42 | time: 0.04 s\n",
      "trigger times: 0\n",
      "Epoch: 1 | Loss: 1.40 | time: 0.05 s\n",
      "trigger times: 1\n",
      "Epoch: 2 | Loss: 1.46 | time: 0.04 s\n",
      "trigger times: 0\n",
      "Epoch: 3 | Loss: 1.43 | time: 0.06 s\n",
      "trigger times: 1\n",
      "Epoch: 4 | Loss: 1.52 | time: 0.03 s\n",
      "trigger times: 0\n",
      "Epoch: 5 | Loss: 1.52 | time: 0.04 s\n",
      "trigger times: 0\n",
      "Epoch: 6 | Loss: 1.39 | time: 0.05 s\n",
      "trigger times: 1\n",
      "Epoch: 7 | Loss: 1.63 | time: 0.05 s\n",
      "trigger times: 0\n",
      "Epoch: 8 | Loss: 1.47 | time: 0.04 s\n",
      "trigger times: 0\n",
      "Epoch: 9 | Loss: 1.40 | time: 0.05 s\n",
      "trigger times: 1\n",
      "Epoch: 10 | Loss: 1.54 | time: 0.04 s\n",
      "trigger times: 0\n",
      "Epoch: 11 | Loss: 1.42 | time: 0.04 s\n",
      "trigger times: 1\n",
      "Epoch: 12 | Loss: 1.59 | time: 0.04 s\n",
      "trigger times: 2\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "All Time: 36.35 s | Loss: 1.76\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, loss_fn)\n",
    "trainer.train(tensor_x, tensor_y, test_x, test_y, epochs, batch_size, epoch_print=True, sampler=sampler)\n",
    "# trainer.test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target train 0/1: 900/100\n",
      "[900 100]\n",
      "batch index 0, 0/1: 55/45\n",
      "batch index 1, 0/1: 54/46\n",
      "batch index 2, 0/1: 52/48\n",
      "batch index 3, 0/1: 60/40\n",
      "batch index 4, 0/1: 46/54\n",
      "batch index 5, 0/1: 44/56\n",
      "batch index 6, 0/1: 43/57\n",
      "batch index 7, 0/1: 54/46\n",
      "batch index 8, 0/1: 57/43\n",
      "batch index 9, 0/1: 52/48\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "numDataPoints = 1000\n",
    "data_dim = 5\n",
    "bs = 100\n",
    "\n",
    "# Create dummy data with class imbalance 9 to 1\n",
    "data = torch.FloatTensor(numDataPoints, data_dim)\n",
    "target = np.hstack((np.zeros(int(numDataPoints * 0.9), dtype=np.int32),\n",
    "                    np.ones(int(numDataPoints * 0.1), dtype=np.int32)))\n",
    "\n",
    "print ('target train 0/1: {}/{}'.format(\n",
    "    len(np.where(target == 0)[0]), len(np.where(target == 1)[0])))\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "print(class_sample_count)\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weigth = samples_weight.double()\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "target = torch.from_numpy(target).long()\n",
    "train_dataset = torch.utils.data.TensorDataset(data, target)\n",
    "\n",
    "train_loader = Data.DataLoader(\n",
    "    train_dataset, batch_size=bs, num_workers=1, sampler=sampler)\n",
    "\n",
    "for i, (data, target) in enumerate(train_loader):\n",
    "    print (\"batch index {}, 0/1: {}/{}\".format(\n",
    "        i,\n",
    "        len(np.where(target.numpy() == 0)[0]),\n",
    "        len(np.where(target.numpy() == 1)[0])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "681206c3b8dadcd0ad9db9cda94afb40592645b096d359128b6de26b00c40796"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('jand_venv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
