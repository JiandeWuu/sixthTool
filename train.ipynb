{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiandewu/.pyenv/versions/3.9.6/envs/testvenv/lib/python3.9/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'T' 'C' 'G' 'A'] ['Cytoplasm' 'Exosome' 'Nucleus' 'Ribosome']\n"
     ]
    }
   ],
   "source": [
    "vocab = np.array(['T', 'C', 'G', 'A'])\n",
    "vocab = np.insert(vocab, 0, '')\n",
    "chunk_tags = np.unique(data[:, 0])\n",
    "print(vocab, chunk_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_mers(data, n):\n",
    "    arrays = []\n",
    "    for s in data:\n",
    "        array = []\n",
    "        for i in range(len(s) - n):\n",
    "            array.append(s[i:i + n].lower())\n",
    "        arrays.append(array)\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,1] = k_mers(data[:,1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(data):\n",
    "    word_counts = Counter(row.lower() for sample in data for row in sample)\n",
    "    vocab = [w for w, f in iter(word_counts.items()) if f >= 2]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(arrays, maxlen, value=0):\n",
    "    mask = []\n",
    "    for i in range(len(arrays)):\n",
    "        arrays[i] = arrays[i][:maxlen]\n",
    "        while(len(arrays[i]) < maxlen):\n",
    "            arrays[i].append(value)\n",
    "        mask.append(np.where(arrays[i] == value))\n",
    "    return arrays, mask\n",
    "\n",
    "def process_data(data, vocab, chunk_tags, maxlen=None, onehot=False):\n",
    "\n",
    "    word2idx = dict((w, i) for i, w in enumerate(vocab)) # 製作 word 2 id array\n",
    "    x = [[word2idx.get(w, 1) for w in s[1]] for s in data]  # set to <unk> (index 1) if not in vocab\n",
    "    \n",
    "    y_chunk = [list(chunk_tags).index(s[0]) for s in data]\n",
    "\n",
    "    x, mask = pad_sequences(x, maxlen)  # left padding\n",
    "    # y_chunk = pad_sequences(y_chunk, maxlen, value=-1)\n",
    "\n",
    "    if onehot:\n",
    "        y_chunk = np.eye(len(chunk_tags), dtype='float32')[y_chunk]\n",
    "    \n",
    "    return x, mask, y_chunk\n",
    "\n",
    "x, mask, y_chunk = process_data(data, vocab, chunk_tags, maxlen=10000, onehot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Bilstm(torch.nn.Module):\n",
    "    \"\"\"BiLSTM Model 的初始化\n",
    "\n",
    "    Args:\n",
    "        nvocab (int): 詞庫 size\n",
    "        ntoken (int): 輸出的類別 size\n",
    "        ninp (int): 詞向量的 size\n",
    "        nhid (int): 隱藏層 size\n",
    "        nlayers (int): LSTM的層數\n",
    "        dropout (float, optional): 如果不為零，則在除最後一層以外的每個LSTM層的輸出上引入一個Dropout層，其丟棄概率等於 dropout . Defaults to 0.0.\n",
    "        batch_first (bool, optional): 如果為True，則將輸入和輸出張量提供為（批次，序列）. Defaults to True.\n",
    "    \"\"\"\n",
    "    def __init__(self, nvocab: int, ntoken: int, ninp: int, nhid: int, nlayers: int, dropout: float=0.0, batch_first: bool=True):\n",
    "        \n",
    "        super(Bilstm, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(nvocab, ninp, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout, bidirectional=True, batch_first=batch_first)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化 embedding, liner 兩層的 weight\n",
    "        \"\"\"\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        \"\"\"運算一次\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): 詞 tensor。如果batch_first=True，x為（批次，序列），否則（序列，批次）。\n",
    "\n",
    "        Returns:\n",
    "            [torch.tensor]: 運算完的3D向量，多了一維類別機率分數。\n",
    "            [torch.tensor]: LSTM層的Hidden weight。\n",
    "        \"\"\"\n",
    "        batch_size = x.size()[0]\n",
    "\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        \n",
    "        hid = self.init_hidden(batch_size)\n",
    "        output, (final_hidden_state, final_cell_state) = self.rnn(emb, hid)\n",
    "        final_hidden_state = self.drop(final_hidden_state)\n",
    "        decoded = self.decoder(final_hidden_state[-1])\n",
    "        return decoded\n",
    "\n",
    "    def init_hidden(self, bsz: int):\n",
    "        \"\"\"取得LSTM shape size 的初始化  weight\n",
    "\n",
    "        Args:\n",
    "            bsz (int): batch_size\n",
    "\n",
    "        Returns:\n",
    "            [tuple]: LSTM shape size 的初始化  weight\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers * 2, bsz, self.nhid), weight.new_zeros(self.nlayers * 2, bsz, self.nhid))\n",
    "    \n",
    "    def predict(self, x: torch.tensor):\n",
    "        \"\"\"預測並輸出機率大的類別\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): 詞 tensor。如果batch_first=True，input shape為（批次，序列），否則（序列，批次）。\n",
    "\n",
    "        Returns:\n",
    "            [torch.tensor]: shape 與 x 一樣，但是序列為類別序列。\n",
    "        \"\"\"\n",
    "        decoded, _  = self.forward(x)\n",
    "        _, output = decoded.max(dim=2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step: 0/15 | Loss: 1.39 | time: 1.96 s\n",
      "Epoch: 0 | Step: 1/15 | Loss: 1.39 | time: 1.91 s\n",
      "Epoch: 0 | Step: 2/15 | Loss: 1.39 | time: 2.05 s\n",
      "Epoch: 0 | Step: 3/15 | Loss: 1.39 | time: 1.98 s\n",
      "Epoch: 0 | Step: 4/15 | Loss: 1.39 | time: 1.97 s\n",
      "Epoch: 0 | Step: 5/15 | Loss: 1.39 | time: 1.97 s\n",
      "Epoch: 0 | Step: 6/15 | Loss: 1.39 | time: 2.13 s\n",
      "Epoch: 0 | Step: 7/15 | Loss: 1.39 | time: 2.25 s\n",
      "Epoch: 0 | Step: 8/15 | Loss: 1.39 | time: 2.04 s\n",
      "Epoch: 0 | Step: 9/15 | Loss: 1.39 | time: 2.32 s\n",
      "Epoch: 0 | Step: 10/15 | Loss: 1.39 | time: 2.13 s\n",
      "Epoch: 0 | Step: 11/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 0 | Step: 12/15 | Loss: 1.38 | time: 2.08 s\n",
      "Epoch: 0 | Step: 13/15 | Loss: 1.38 | time: 2.09 s\n",
      "Epoch: 0 | Step: 14/15 | Loss: 1.39 | time: 1.06 s\n",
      "Epoch: 1 | Step: 0/15 | Loss: 1.38 | time: 2.11 s\n",
      "Epoch: 1 | Step: 1/15 | Loss: 1.38 | time: 2.08 s\n",
      "Epoch: 1 | Step: 2/15 | Loss: 1.39 | time: 2.08 s\n",
      "Epoch: 1 | Step: 3/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 1 | Step: 4/15 | Loss: 1.38 | time: 2.12 s\n",
      "Epoch: 1 | Step: 5/15 | Loss: 1.38 | time: 2.11 s\n",
      "Epoch: 1 | Step: 6/15 | Loss: 1.38 | time: 2.08 s\n",
      "Epoch: 1 | Step: 7/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 1 | Step: 8/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 1 | Step: 9/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 1 | Step: 10/15 | Loss: 1.38 | time: 2.09 s\n",
      "Epoch: 1 | Step: 11/15 | Loss: 1.38 | time: 2.09 s\n",
      "Epoch: 1 | Step: 12/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 1 | Step: 13/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 1 | Step: 14/15 | Loss: 1.39 | time: 1.06 s\n",
      "Epoch: 2 | Step: 0/15 | Loss: 1.38 | time: 2.11 s\n",
      "Epoch: 2 | Step: 1/15 | Loss: 1.38 | time: 2.11 s\n",
      "Epoch: 2 | Step: 2/15 | Loss: 1.38 | time: 2.09 s\n",
      "Epoch: 2 | Step: 3/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 2 | Step: 4/15 | Loss: 1.38 | time: 2.09 s\n",
      "Epoch: 2 | Step: 5/15 | Loss: 1.38 | time: 2.08 s\n",
      "Epoch: 2 | Step: 6/15 | Loss: 1.38 | time: 2.07 s\n",
      "Epoch: 2 | Step: 7/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 2 | Step: 8/15 | Loss: 1.38 | time: 2.16 s\n",
      "Epoch: 2 | Step: 9/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 2 | Step: 10/15 | Loss: 1.38 | time: 2.09 s\n",
      "Epoch: 2 | Step: 11/15 | Loss: 1.38 | time: 2.10 s\n",
      "Epoch: 2 | Step: 12/15 | Loss: 1.38 | time: 2.21 s\n",
      "Epoch: 2 | Step: 13/15 | Loss: 1.38 | time: 2.17 s\n",
      "Epoch: 2 | Step: 14/15 | Loss: 1.39 | time: 1.09 s\n",
      "Epoch: 3 | Step: 0/15 | Loss: 1.38 | time: 2.14 s\n",
      "Epoch: 3 | Step: 1/15 | Loss: 1.38 | time: 2.12 s\n",
      "Epoch: 3 | Step: 2/15 | Loss: 1.38 | time: 2.14 s\n",
      "Epoch: 3 | Step: 3/15 | Loss: 1.37 | time: 2.19 s\n",
      "Epoch: 3 | Step: 4/15 | Loss: 1.38 | time: 2.18 s\n",
      "Epoch: 3 | Step: 5/15 | Loss: 1.38 | time: 2.15 s\n",
      "Epoch: 3 | Step: 6/15 | Loss: 1.38 | time: 2.11 s\n",
      "Epoch: 3 | Step: 7/15 | Loss: 1.37 | time: 2.24 s\n",
      "Epoch: 3 | Step: 8/15 | Loss: 1.38 | time: 2.14 s\n",
      "Epoch: 3 | Step: 9/15 | Loss: 1.37 | time: 2.21 s\n",
      "Epoch: 3 | Step: 10/15 | Loss: 1.38 | time: 2.37 s\n",
      "Epoch: 3 | Step: 11/15 | Loss: 1.37 | time: 2.09 s\n",
      "Epoch: 3 | Step: 12/15 | Loss: 1.38 | time: 2.17 s\n",
      "Epoch: 3 | Step: 13/15 | Loss: 1.37 | time: 2.19 s\n",
      "Epoch: 3 | Step: 14/15 | Loss: 1.38 | time: 1.07 s\n",
      "Epoch: 4 | Step: 0/15 | Loss: 1.38 | time: 2.14 s\n",
      "Epoch: 4 | Step: 1/15 | Loss: 1.37 | time: 2.14 s\n",
      "Epoch: 4 | Step: 2/15 | Loss: 1.38 | time: 2.13 s\n",
      "Epoch: 4 | Step: 3/15 | Loss: 1.36 | time: 2.16 s\n",
      "Epoch: 4 | Step: 4/15 | Loss: 1.37 | time: 2.19 s\n",
      "Epoch: 4 | Step: 5/15 | Loss: 1.37 | time: 2.12 s\n",
      "Epoch: 4 | Step: 6/15 | Loss: 1.37 | time: 2.12 s\n",
      "Epoch: 4 | Step: 7/15 | Loss: 1.37 | time: 2.12 s\n",
      "Epoch: 4 | Step: 8/15 | Loss: 1.37 | time: 2.15 s\n",
      "Epoch: 4 | Step: 9/15 | Loss: 1.37 | time: 2.14 s\n",
      "Epoch: 4 | Step: 10/15 | Loss: 1.37 | time: 2.15 s\n",
      "Epoch: 4 | Step: 11/15 | Loss: 1.37 | time: 2.14 s\n",
      "Epoch: 4 | Step: 12/15 | Loss: 1.37 | time: 2.13 s\n",
      "Epoch: 4 | Step: 13/15 | Loss: 1.36 | time: 2.13 s\n",
      "Epoch: 4 | Step: 14/15 | Loss: 1.38 | time: 1.08 s\n",
      "Epoch: 5 | Step: 0/15 | Loss: 1.37 | time: 2.16 s\n",
      "Epoch: 5 | Step: 1/15 | Loss: 1.35 | time: 2.13 s\n",
      "Epoch: 5 | Step: 2/15 | Loss: 1.37 | time: 2.13 s\n",
      "Epoch: 5 | Step: 3/15 | Loss: 1.35 | time: 2.13 s\n",
      "Epoch: 5 | Step: 4/15 | Loss: 1.36 | time: 2.13 s\n",
      "Epoch: 5 | Step: 5/15 | Loss: 1.36 | time: 2.42 s\n",
      "Epoch: 5 | Step: 6/15 | Loss: 1.36 | time: 2.13 s\n",
      "Epoch: 5 | Step: 7/15 | Loss: 1.35 | time: 2.13 s\n",
      "Epoch: 5 | Step: 8/15 | Loss: 1.36 | time: 2.13 s\n",
      "Epoch: 5 | Step: 9/15 | Loss: 1.35 | time: 2.13 s\n",
      "Epoch: 5 | Step: 10/15 | Loss: 1.36 | time: 2.15 s\n",
      "Epoch: 5 | Step: 11/15 | Loss: 1.35 | time: 2.13 s\n",
      "Epoch: 5 | Step: 12/15 | Loss: 1.36 | time: 2.14 s\n",
      "Epoch: 5 | Step: 13/15 | Loss: 1.34 | time: 2.13 s\n",
      "Epoch: 5 | Step: 14/15 | Loss: 1.40 | time: 1.09 s\n",
      "Epoch: 6 | Step: 0/15 | Loss: 1.35 | time: 2.15 s\n",
      "Epoch: 6 | Step: 1/15 | Loss: 1.32 | time: 2.13 s\n",
      "Epoch: 6 | Step: 2/15 | Loss: 1.37 | time: 2.14 s\n",
      "Epoch: 6 | Step: 3/15 | Loss: 1.31 | time: 2.14 s\n",
      "Epoch: 6 | Step: 4/15 | Loss: 1.33 | time: 2.14 s\n",
      "Epoch: 6 | Step: 5/15 | Loss: 1.33 | time: 2.12 s\n",
      "Epoch: 6 | Step: 6/15 | Loss: 1.35 | time: 2.13 s\n",
      "Epoch: 6 | Step: 7/15 | Loss: 1.32 | time: 2.13 s\n",
      "Epoch: 6 | Step: 8/15 | Loss: 1.34 | time: 2.17 s\n",
      "Epoch: 6 | Step: 9/15 | Loss: 1.32 | time: 2.16 s\n",
      "Epoch: 6 | Step: 10/15 | Loss: 1.35 | time: 2.14 s\n",
      "Epoch: 6 | Step: 11/15 | Loss: 1.33 | time: 2.14 s\n",
      "Epoch: 6 | Step: 12/15 | Loss: 1.34 | time: 2.14 s\n",
      "Epoch: 6 | Step: 13/15 | Loss: 1.32 | time: 2.15 s\n",
      "Epoch: 6 | Step: 14/15 | Loss: 1.43 | time: 1.15 s\n",
      "Epoch: 7 | Step: 0/15 | Loss: 1.34 | time: 2.21 s\n",
      "Epoch: 7 | Step: 1/15 | Loss: 1.29 | time: 2.13 s\n",
      "Epoch: 7 | Step: 2/15 | Loss: 1.36 | time: 2.13 s\n",
      "Epoch: 7 | Step: 3/15 | Loss: 1.28 | time: 2.15 s\n",
      "Epoch: 7 | Step: 4/15 | Loss: 1.32 | time: 2.18 s\n",
      "Epoch: 7 | Step: 5/15 | Loss: 1.31 | time: 2.15 s\n",
      "Epoch: 7 | Step: 6/15 | Loss: 1.34 | time: 2.16 s\n",
      "Epoch: 7 | Step: 7/15 | Loss: 1.30 | time: 2.14 s\n",
      "Epoch: 7 | Step: 8/15 | Loss: 1.33 | time: 2.17 s\n",
      "Epoch: 7 | Step: 9/15 | Loss: 1.30 | time: 2.16 s\n",
      "Epoch: 7 | Step: 10/15 | Loss: 1.35 | time: 2.15 s\n",
      "Epoch: 7 | Step: 11/15 | Loss: 1.32 | time: 2.14 s\n",
      "Epoch: 7 | Step: 12/15 | Loss: 1.34 | time: 2.35 s\n",
      "Epoch: 7 | Step: 13/15 | Loss: 1.30 | time: 2.16 s\n",
      "Epoch: 7 | Step: 14/15 | Loss: 1.46 | time: 1.08 s\n",
      "Epoch: 8 | Step: 0/15 | Loss: 1.33 | time: 2.20 s\n",
      "Epoch: 8 | Step: 1/15 | Loss: 1.27 | time: 2.22 s\n",
      "Epoch: 8 | Step: 2/15 | Loss: 1.36 | time: 2.18 s\n",
      "Epoch: 8 | Step: 3/15 | Loss: 1.26 | time: 2.19 s\n",
      "Epoch: 8 | Step: 4/15 | Loss: 1.30 | time: 2.17 s\n",
      "Epoch: 8 | Step: 5/15 | Loss: 1.30 | time: 2.16 s\n",
      "Epoch: 8 | Step: 6/15 | Loss: 1.33 | time: 2.17 s\n",
      "Epoch: 8 | Step: 7/15 | Loss: 1.29 | time: 2.17 s\n",
      "Epoch: 8 | Step: 8/15 | Loss: 1.32 | time: 2.16 s\n",
      "Epoch: 8 | Step: 9/15 | Loss: 1.29 | time: 2.20 s\n",
      "Epoch: 8 | Step: 10/15 | Loss: 1.35 | time: 2.20 s\n",
      "Epoch: 8 | Step: 11/15 | Loss: 1.31 | time: 2.17 s\n",
      "Epoch: 8 | Step: 12/15 | Loss: 1.34 | time: 2.17 s\n",
      "Epoch: 8 | Step: 13/15 | Loss: 1.29 | time: 2.17 s\n",
      "Epoch: 8 | Step: 14/15 | Loss: 1.48 | time: 1.08 s\n",
      "Epoch: 9 | Step: 0/15 | Loss: 1.32 | time: 2.22 s\n",
      "Epoch: 9 | Step: 1/15 | Loss: 1.26 | time: 2.16 s\n",
      "Epoch: 9 | Step: 2/15 | Loss: 1.36 | time: 2.16 s\n",
      "Epoch: 9 | Step: 3/15 | Loss: 1.24 | time: 2.16 s\n",
      "Epoch: 9 | Step: 4/15 | Loss: 1.30 | time: 2.18 s\n",
      "Epoch: 9 | Step: 5/15 | Loss: 1.29 | time: 2.17 s\n",
      "Epoch: 9 | Step: 6/15 | Loss: 1.33 | time: 2.17 s\n",
      "Epoch: 9 | Step: 7/15 | Loss: 1.28 | time: 2.17 s\n",
      "Epoch: 9 | Step: 8/15 | Loss: 1.32 | time: 2.16 s\n",
      "Epoch: 9 | Step: 9/15 | Loss: 1.29 | time: 2.17 s\n",
      "Epoch: 9 | Step: 10/15 | Loss: 1.35 | time: 2.18 s\n",
      "Epoch: 9 | Step: 11/15 | Loss: 1.31 | time: 2.16 s\n",
      "Epoch: 9 | Step: 12/15 | Loss: 1.34 | time: 2.20 s\n",
      "Epoch: 9 | Step: 13/15 | Loss: 1.29 | time: 2.17 s\n",
      "Epoch: 9 | Step: 14/15 | Loss: 1.49 | time: 1.14 s\n",
      "Epoch: 10 | Step: 0/15 | Loss: 1.32 | time: 2.18 s\n",
      "Epoch: 10 | Step: 1/15 | Loss: 1.25 | time: 2.16 s\n",
      "Epoch: 10 | Step: 2/15 | Loss: 1.36 | time: 2.15 s\n",
      "Epoch: 10 | Step: 3/15 | Loss: 1.24 | time: 2.17 s\n",
      "Epoch: 10 | Step: 4/15 | Loss: 1.29 | time: 2.17 s\n",
      "Epoch: 10 | Step: 5/15 | Loss: 1.29 | time: 2.16 s\n",
      "Epoch: 10 | Step: 6/15 | Loss: 1.33 | time: 2.28 s\n",
      "Epoch: 10 | Step: 7/15 | Loss: 1.27 | time: 2.26 s\n",
      "Epoch: 10 | Step: 8/15 | Loss: 1.31 | time: 2.17 s\n",
      "Epoch: 10 | Step: 9/15 | Loss: 1.28 | time: 2.17 s\n",
      "Epoch: 10 | Step: 10/15 | Loss: 1.35 | time: 2.18 s\n",
      "Epoch: 10 | Step: 11/15 | Loss: 1.31 | time: 2.17 s\n",
      "Epoch: 10 | Step: 12/15 | Loss: 1.34 | time: 2.18 s\n",
      "Epoch: 10 | Step: 13/15 | Loss: 1.28 | time: 2.17 s\n",
      "Epoch: 10 | Step: 14/15 | Loss: 1.50 | time: 1.10 s\n",
      "Epoch: 11 | Step: 0/15 | Loss: 1.32 | time: 2.19 s\n",
      "Epoch: 11 | Step: 1/15 | Loss: 1.24 | time: 2.16 s\n",
      "Epoch: 11 | Step: 2/15 | Loss: 1.36 | time: 2.17 s\n",
      "Epoch: 11 | Step: 3/15 | Loss: 1.23 | time: 2.21 s\n",
      "Epoch: 11 | Step: 4/15 | Loss: 1.29 | time: 2.15 s\n",
      "Epoch: 11 | Step: 5/15 | Loss: 1.29 | time: 2.17 s\n",
      "Epoch: 11 | Step: 6/15 | Loss: 1.33 | time: 2.19 s\n",
      "Epoch: 11 | Step: 7/15 | Loss: 1.27 | time: 2.15 s\n",
      "Epoch: 11 | Step: 8/15 | Loss: 1.31 | time: 2.16 s\n",
      "Epoch: 11 | Step: 9/15 | Loss: 1.28 | time: 2.15 s\n",
      "Epoch: 11 | Step: 10/15 | Loss: 1.35 | time: 2.14 s\n",
      "Epoch: 11 | Step: 11/15 | Loss: 1.30 | time: 2.15 s\n",
      "Epoch: 11 | Step: 12/15 | Loss: 1.34 | time: 2.14 s\n",
      "Epoch: 11 | Step: 13/15 | Loss: 1.28 | time: 2.15 s\n",
      "Epoch: 11 | Step: 14/15 | Loss: 1.50 | time: 1.08 s\n",
      "Epoch: 12 | Step: 0/15 | Loss: 1.32 | time: 2.16 s\n",
      "Epoch: 12 | Step: 1/15 | Loss: 1.24 | time: 2.16 s\n",
      "Epoch: 12 | Step: 2/15 | Loss: 1.36 | time: 2.16 s\n",
      "Epoch: 12 | Step: 3/15 | Loss: 1.23 | time: 2.16 s\n",
      "Epoch: 12 | Step: 4/15 | Loss: 1.29 | time: 2.18 s\n",
      "Epoch: 12 | Step: 5/15 | Loss: 1.28 | time: 2.16 s\n",
      "Epoch: 12 | Step: 6/15 | Loss: 1.33 | time: 2.18 s\n",
      "Epoch: 12 | Step: 7/15 | Loss: 1.27 | time: 2.18 s\n",
      "Epoch: 12 | Step: 8/15 | Loss: 1.31 | time: 2.18 s\n",
      "Epoch: 12 | Step: 9/15 | Loss: 1.28 | time: 2.17 s\n",
      "Epoch: 12 | Step: 10/15 | Loss: 1.35 | time: 2.15 s\n",
      "Epoch: 12 | Step: 11/15 | Loss: 1.30 | time: 2.18 s\n",
      "Epoch: 12 | Step: 12/15 | Loss: 1.34 | time: 2.19 s\n",
      "Epoch: 12 | Step: 13/15 | Loss: 1.28 | time: 2.24 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x_/6bxyk_hs419bw4v7wqlycrb00000gn/T/ipykernel_94117/2537312105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: %i | Step: %i/%i | Loss: %.2f | time: %.2f s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.6/envs/testvenv/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.6/envs/testvenv/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nvocab = len(vocab)\n",
    "ntoken = len(chunk_tags)\n",
    "\n",
    "ninp = 32\n",
    "nhid = 32\n",
    "nlayers = 2\n",
    "\n",
    "model = Bilstm(nvocab, ntoken, ninp, nhid, nlayers)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "tensor_x = torch.tensor(x).to(torch.int64)\n",
    "tensor_mask = torch.tensor(mask).to(torch.int64)\n",
    "tensor_y = torch.tensor(y_chunk).to(torch.int64)\n",
    "\n",
    "\n",
    "dataset = Data.TensorDataset(tensor_x, tensor_mask, tensor_y)\n",
    "\n",
    "train_set, valid_set = Data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "    dataset = train_set,\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "step_size = len(loader)\n",
    "for epoch in range(epochs):\n",
    "    for step, (batch_x, batch_mask, batch_y) in enumerate(loader):\n",
    "        step_time = time.time()\n",
    "        model.zero_grad()\n",
    "        pred = model(batch_x)\n",
    "        \n",
    "        loss = loss_fn(pred.softmax(dim=1), batch_y)\n",
    "        print('Epoch: %i | Step: %i/%i | Loss: %.2f | time: %.2f s' % (epoch, step, step_size, loss, time.time() - step_time))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('all time : ', time.time() - start_time,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list = list(df['Seq'])\n",
    "loc_list = list(df['sublocation'])\n",
    "for i in range(len(df.index)):\n",
    "    df['Seq'][i] = hashlib.md5(df['Seq'][i].encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/data_MD5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "911519d9d2a6bb910d202bb6a963e5f76f777f159da93d44a4a099157dae11b7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('testvenv': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
